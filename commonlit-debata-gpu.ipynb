{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install spacy\n# !pip install datasets\n# !pip install transformers\n# !pip install sentencepiece\n","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:04:13.458479Z","iopub.execute_input":"2023-08-19T11:04:13.459017Z","iopub.status.idle":"2023-08-19T11:04:13.463938Z","shell.execute_reply.started":"2023-08-19T11:04:13.458982Z","shell.execute_reply":"2023-08-19T11:04:13.462910Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom transformers import AutoTokenizer, AutoConfig, TFAutoModel\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling1D,GlobalMaxPooling1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nimport pickle\nimport os\nfrom tensorflow.keras.backend import clear_session\n\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-19T16:51:51.087888Z","iopub.execute_input":"2023-08-19T16:51:51.088282Z","iopub.status.idle":"2023-08-19T16:51:51.115459Z","shell.execute_reply.started":"2023-08-19T16:51:51.088250Z","shell.execute_reply":"2023-08-19T16:51:51.114541Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"path = \"/kaggle/working/models/\"\n# Check whether the specified path exists or not\nisExist = os.path.exists(path)\nif not isExist:\n\n   # Create a new directory because it does not exist\n   os.makedirs(path)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:04:13.542170Z","iopub.execute_input":"2023-08-19T11:04:13.542434Z","iopub.status.idle":"2023-08-19T11:04:13.548047Z","shell.execute_reply.started":"2023-08-19T11:04:13.542411Z","shell.execute_reply":"2023-08-19T11:04:13.546942Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"path_dir = \"/kaggle/input/commonlit-evaluate-student-summaries\"\nPrompts_train = pd.read_csv(f'{path_dir}/prompts_train.csv')\nPrompts_test = pd.read_csv(f'{path_dir}/prompts_test.csv')\nSummaries_train = pd.read_csv(f'{path_dir}/summaries_train.csv')\nSummaries_test = pd.read_csv(f'{path_dir}/summaries_test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-08-19T16:53:46.152040Z","iopub.execute_input":"2023-08-19T16:53:46.152398Z","iopub.status.idle":"2023-08-19T16:53:46.273573Z","shell.execute_reply.started":"2023-08-19T16:53:46.152370Z","shell.execute_reply":"2023-08-19T16:53:46.272394Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"Summaries_train = Summaries_train.merge(Prompts_train, on=\"prompt_id\", how=\"left\")\nSummaries_test = Summaries_test.merge(Prompts_test, on=\"prompt_id\", how=\"left\")","metadata":{"execution":{"iopub.status.busy":"2023-08-19T16:53:47.612670Z","iopub.execute_input":"2023-08-19T16:53:47.613064Z","iopub.status.idle":"2023-08-19T16:53:47.644536Z","shell.execute_reply.started":"2023-08-19T16:53:47.613017Z","shell.execute_reply":"2023-08-19T16:53:47.643614Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def clean_data(text):\n    text = str(text).lower()\n    text = re.sub(r'[\\n\\r]', '', str(text))\n    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n    text = re.sub(' +', ' ', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = ' '.join([word for word in text.split() if word not in STOP_WORDS])\n    return text\n\ndef get_max_length(df, column):\n    max_length = df[column].apply(lambda x: len(x.split())).max()\n    return max_length\n\ndef select_features(df,x_features, y_features):\n    X = df[x_features].values\n    y = df[y_features].values\n    return X, y\n\ndef deberta_tokenizer(model_name):\n    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, add_prefix_space=True)\n    return tokenizer\n\ndef deberta_encode(texts, tokenizer, MAX_LENGTH):\n    input_ids = []\n    attention_mask = []\n    \n    for text in texts.tolist():\n        token = tokenizer(text, \n                          add_special_tokens=True, \n                          max_length=MAX_LENGTH, \n                          return_attention_mask=True, \n                          return_tensors=\"np\", \n                          truncation=True, \n                          padding='max_length')\n        input_ids.append(token['input_ids'][0])\n        attention_mask.append(token['attention_mask'][0])\n    return np.array(input_ids, dtype=\"int32\"), np.array(attention_mask, dtype=\"int32\")\n\ndef get_dataset(df,input_ids,attention_mask):\n    inputs = np.concatenate((input_ids, attention_mask), axis=1)\n    targets = np.array(df, dtype=\"float32\")\n    return inputs, targets\n\ndef MCRMSE(y_trues, y_preds):\n    scores = []\n    idxes = y_trues.shape[1]\n    for i in range(idxes):\n        y_true = y_trues[:,i]\n        y_pred = y_preds[:,i]\n        score = mean_squared_error(y_true, y_pred, squared=False)\n        scores.append(score)\n    mcrmse_score = np.mean(scores)\n    return mcrmse_score, scores\n\ndef score_loss(y_trues, y_preds):\n    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n    return {\n        'mcrmse_score' : mcrmse_score,\n        'Content_score' : scores[0],\n        'Wording_score' : scores[1]\n    }\n\ndef split_Kfold(data,splits):\n    kf = KFold(n_splits=splits, shuffle=True, random_state=42)\n    split = kf.split(data)\n   \n    for train_index, test_index in split:\n        train_data = data.iloc[train_index]\n        test_data = data.iloc[test_index]\n        yield train_data, test_data","metadata":{"execution":{"iopub.status.busy":"2023-08-19T16:54:57.984086Z","iopub.execute_input":"2023-08-19T16:54:57.984464Z","iopub.status.idle":"2023-08-19T16:54:58.000801Z","shell.execute_reply.started":"2023-08-19T16:54:57.984434Z","shell.execute_reply":"2023-08-19T16:54:57.998862Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"Summaries_train['text'] = Summaries_train['text'].apply(lambda x: clean_data(x))\nSummaries_train['prompt_question'] = Summaries_train['prompt_question'].apply(lambda x: clean_data(x))\nSummaries_train['prompt_title'] = Summaries_train['prompt_title'].apply(lambda x: clean_data(x))\nSummaries_train['prompt_text'] = Summaries_train['prompt_text'].apply(lambda x: clean_data(x))","metadata":{"execution":{"iopub.status.busy":"2023-08-19T16:55:03.199127Z","iopub.execute_input":"2023-08-19T16:55:03.199490Z","iopub.status.idle":"2023-08-19T16:55:14.878187Z","shell.execute_reply.started":"2023-08-19T16:55:03.199462Z","shell.execute_reply":"2023-08-19T16:55:14.877123Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class CONFIG:\n    model_name = \"/kaggle/input/debertav3base\"\n    seed = 42\n    batch_size = 4\n    epochs = 10\n    max_length = 100\n    learning_rate = 2e-5\n    dropout = 0.5\n    n_splits = 4\n    shuffle = True\n    device = tf.config.list_physical_devices('GPU')\n    train_cols = ['text','prompt_question','prompt_title','prompt_text']\n    test_cols = ['content', 'wording']\n    max_length = get_max_length(Summaries_train, 'prompt_text')\n    decay_steps= 2800\n    decay_rate = 0.2\n    ","metadata":{"execution":{"iopub.status.busy":"2023-08-19T16:55:15.011528Z","iopub.execute_input":"2023-08-19T16:55:15.011919Z","iopub.status.idle":"2023-08-19T16:55:15.139303Z","shell.execute_reply.started":"2023-08-19T16:55:15.011884Z","shell.execute_reply":"2023-08-19T16:55:15.138393Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# class CustomDebertaV2Embeddings(layers.Layer):\n#     def __init__(self, config, **kwargs):\n#         super(CustomDebertaV2Embeddings, self).__init__(**kwargs)\n#         self.embeddings = TFDebertaV2Embeddings(config, name=\"embeddings\")  # Use TFDebertaV2Embeddings here\n\n#     def call(self, input_ids=None, position_ids=None, token_type_ids=None, inputs_embeds=None, mask=None, training=False):\n#         if mask is not None:\n#             if mask.dtype == tf.float32:  # Check if the mask is in float32 format\n#                 mask = tf.cast(mask, dtype=tf.float16)  # Cast mask to float16 if it's in float32 format\n#             mask = tf.expand_dims(mask, axis=2)  # Add an extra dimension for element-wise multiplication\n#             final_embeddings = self.embeddings(  # Use self.embeddings to calculate embeddings\n#                 input_ids=input_ids,\n#                 position_ids=position_ids,\n#                 token_type_ids=token_type_ids,\n#                 inputs_embeds=inputs_embeds,\n#                 mask=mask,\n#                 training=training,\n#             )\n#             final_embeddings = final_embeddings * mask\n\n#         return final_embeddings\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:04:27.656117Z","iopub.execute_input":"2023-08-19T11:04:27.656474Z","iopub.status.idle":"2023-08-19T11:04:27.674152Z","shell.execute_reply.started":"2023-08-19T11:04:27.656443Z","shell.execute_reply":"2023-08-19T11:04:27.670942Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def get_model(model_name, MAX_LENGTH):\n    if CONFIG.device:\n        with tf.device('/GPU:0'):\n            config = AutoConfig.from_pretrained(model_name)\n            config.update({'output_hidden_states': True, \n                        'hidden_dropout_prob': 0.005,\n                        'layer_norm_eps': 1e-7,\n                        'num_labels': 2,\n                        \"problem_type\": \"regression\"})\n\n            deberta_model = TFAutoModel.from_pretrained(model_name, config=config)\n\n            input_ids = Input(shape=(MAX_LENGTH,), dtype=tf.int32, name=\"input_ids\")\n            attention_mask = Input(shape=(MAX_LENGTH,), dtype=tf.int32, name=\"attention_mask\")\n\n            sequence_output = deberta_model.deberta(input_ids, attention_mask=attention_mask)\n            #sequence_output_layer = sequence_output.last_hidden_state[:, 0, :]\n            #print(sequence_output_layer)\n            max_pooling = GlobalAveragePooling1D()(sequence_output.last_hidden_state)\n            dense_layer = Dense(2, activation=\"sigmoid\")(max_pooling)\n            output = layers.Rescaling(scale=6.0, offset=-2.0)(dense_layer)\n            model = Model(inputs=[input_ids, attention_mask], outputs=output)\n            lr_schedules = tf.keras.optimizers.schedules.ExponentialDecay(\n            initial_learning_rate=CONFIG.learning_rate, \n            decay_steps=CONFIG.decay_steps, \n            decay_rate=CONFIG.decay_rate)\n            model.compile(optimizer=Adam(learning_rate=CONFIG.learning_rate), loss='mse', metrics=['mse'])\n\n    return model\n\ndef scheduler(epoch):\n    learning_rate = CONFIG.learning_rate\n    if epoch == 0:\n        return learning_rate\n    else:\n        return learning_rate * (0.9**epoch)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:04:27.675219Z","iopub.execute_input":"2023-08-19T11:04:27.676910Z","iopub.status.idle":"2023-08-19T11:04:27.703385Z","shell.execute_reply.started":"2023-08-19T11:04:27.676856Z","shell.execute_reply":"2023-08-19T11:04:27.695352Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"MCRMSE_scores = []\n\ndef train_and_save_model(index,train_data, valid_data):\n        tokenizer = deberta_tokenizer(CONFIG.model_name)\n\n        # select features\n        X_train, y_train = select_features(train_data, CONFIG.train_cols, CONFIG.test_cols)\n        X_valid, y_valid = select_features(valid_data, CONFIG.train_cols, CONFIG.test_cols)\n\n        # tokenize\n        input_ids_train, attention_masks_train = deberta_encode(X_train, tokenizer, CONFIG.max_length)\n        input_ids_valid, attention_masks_valid = deberta_encode(X_valid, tokenizer, CONFIG.max_length)\n\n        # create dataset\n        inputs_trains = [input_ids_train, attention_masks_train]\n        targets_trains = np.array(y_train, dtype=\"float32\")\n\n        inputs_valids = [input_ids_valid, attention_masks_valid]\n        targets_valids = np.array(y_valid, dtype=\"float32\")\n\n       \n        tf.keras.backend.clear_session()\n        # fit model\n        model = get_model(CONFIG.model_name, CONFIG.max_length)\n        print(model.summary())\n        callbacks = [\n        tf.keras.callbacks.ModelCheckpoint(f\"{path}/model_fold_{index}.ckpt\",\n                                           monitor=\"val_loss\",\n                                           mode=\"min\",\n                                           save_best_only=True,\n                                           verbose=1,\n                                           save_weights_only=True,),\n        tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n                                         min_delta=1e-5, \n                                         patience=2, \n                                         verbose=1,\n                                         mode='min',\n                                        restore_best_weights=True), tf.keras.callbacks.LearningRateScheduler(scheduler)\n        ]\n\n        # Validation and other steps\n        history = model.fit(\n            inputs_trains,\n            targets_trains,\n            validation_data=(inputs_valids, targets_valids),\n            epochs=CONFIG.epochs, \n            batch_size=CONFIG.batch_size,\n            verbose=1,\n            callbacks = callbacks\n        )\n        tf.keras.backend.clear_session()\n    \n        # evaluate model\n        model.evaluate(inputs_valids, targets_valids)\n\n        # Score model\n        y_pred = model.predict(inputs_valids)\n\n        # MCRMSE score\n        print('MCRMSE score: ', MCRMSE(y_valid, y_pred))\n        print('Score loss: ', score_loss(y_valid, y_pred))\n        MCRMSE_scores.append(MCRMSE(y_valid, y_pred))\n        \n        model.save(f'{path}/model_{index}_{CONFIG.model_name}') \n#         tf.keras.saving.save_model(\n#     model, filepath, overwrite=True, save_format=None, **kwargs\n# )","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:04:27.705226Z","iopub.execute_input":"2023-08-19T11:04:27.705802Z","iopub.status.idle":"2023-08-19T11:04:27.725789Z","shell.execute_reply.started":"2023-08-19T11:04:27.705770Z","shell.execute_reply":"2023-08-19T11:04:27.724913Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"for gpu in CONFIG.device:\n    tf.config.experimental.set_memory_growth(gpu, True)\n\nkf = KFold(n_splits=CONFIG.n_splits, shuffle=CONFIG.shuffle, random_state=CONFIG.seed)\nfor fold, (train_index, valid_index) in enumerate(kf.split(Summaries_train, groups = Summaries_train['prompt_id'])):\n    tf.keras.backend.clear_session()\n    train_data = Summaries_train.iloc[train_index]\n    valid_data = Summaries_train.iloc[valid_index]\n\n    print(f\"Training Fold {fold}\")\n    train_and_save_model(fold,train_data, valid_data)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:04:27.730167Z","iopub.execute_input":"2023-08-19T11:04:27.732988Z","iopub.status.idle":"2023-08-19T16:38:40.679144Z","shell.execute_reply.started":"2023-08-19T11:04:27.732953Z","shell.execute_reply":"2023-08-19T16:38:40.666802Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Training Fold 0\n","output_type":"stream"},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nAll model checkpoint layers were used when initializing TFDebertaV2Model.\n\nAll the layers of TFDebertaV2Model were initialized from the model checkpoint at /kaggle/input/debertav3base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDebertaV2Model for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_ids (InputLayer)         [(None, 360)]        0           []                               \n                                                                                                  \n attention_mask (InputLayer)    [(None, 360)]        0           []                               \n                                                                                                  \n deberta (TFDebertaV2MainLayer)  TFBaseModelOutput(l  183831552  ['input_ids[0][0]',              \n                                ast_hidden_state=(N               'attention_mask[0][0]']         \n                                one, 360, 768),                                                   \n                                 hidden_states=((No                                               \n                                ne, 360, 768),                                                    \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768)),                                               \n                                 attentions=None)                                                 \n                                                                                                  \n global_average_pooling1d (Glob  (None, 768)         0           ['deberta[0][13]']               \n alAveragePooling1D)                                                                              \n                                                                                                  \n dense (Dense)                  (None, 2)            1538        ['global_average_pooling1d[0][0]'\n                                                                 ]                                \n                                                                                                  \n rescaling (Rescaling)          (None, 2)            0           ['dense[0][0]']                  \n                                                                                                  \n==================================================================================================\nTotal params: 183,833,090\nTrainable params: 183,833,090\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\nEpoch 1/10\n1344/1344 [==============================] - ETA: 0s - loss: 0.5057 - mse: 0.5057\nEpoch 1: val_loss improved from inf to 0.34781, saving model to /kaggle/working/models/model_fold_0.ckpt\n1344/1344 [==============================] - 1024s 708ms/step - loss: 0.5057 - mse: 0.5057 - val_loss: 0.3478 - val_mse: 0.3478 - lr: 2.0000e-05\nEpoch 2/10\n1344/1344 [==============================] - ETA: 0s - loss: 0.3310 - mse: 0.3310\nEpoch 2: val_loss improved from 0.34781 to 0.30959, saving model to /kaggle/working/models/model_fold_0.ckpt\n1344/1344 [==============================] - 922s 686ms/step - loss: 0.3310 - mse: 0.3310 - val_loss: 0.3096 - val_mse: 0.3096 - lr: 1.8000e-05\nEpoch 3/10\n1344/1344 [==============================] - ETA: 0s - loss: 0.2439 - mse: 0.2439\nEpoch 3: val_loss did not improve from 0.30959\n1344/1344 [==============================] - 905s 673ms/step - loss: 0.2439 - mse: 0.2439 - val_loss: 0.3135 - val_mse: 0.3135 - lr: 1.6200e-05\nEpoch 4/10\n1344/1344 [==============================] - ETA: 0s - loss: 0.1826 - mse: 0.1826\nEpoch 4: val_loss improved from 0.30959 to 0.29910, saving model to /kaggle/working/models/model_fold_0.ckpt\n1344/1344 [==============================] - 938s 698ms/step - loss: 0.1826 - mse: 0.1826 - val_loss: 0.2991 - val_mse: 0.2991 - lr: 1.4580e-05\nEpoch 5/10\n1344/1344 [==============================] - ETA: 0s - loss: 0.1319 - mse: 0.1319\nEpoch 5: val_loss did not improve from 0.29910\n1344/1344 [==============================] - 905s 673ms/step - loss: 0.1319 - mse: 0.1319 - val_loss: 0.3269 - val_mse: 0.3269 - lr: 1.3122e-05\nEpoch 6/10\n1344/1344 [==============================] - ETA: 0s - loss: 0.0982 - mse: 0.0982\nEpoch 6: val_loss did not improve from 0.29910\nRestoring model weights from the end of the best epoch: 4.\n1344/1344 [==============================] - 904s 673ms/step - loss: 0.0982 - mse: 0.0982 - val_loss: 0.3031 - val_mse: 0.3031 - lr: 1.1810e-05\nEpoch 6: early stopping\n56/56 [==============================] - 69s 1s/step - loss: 0.2991 - mse: 0.2991\n56/56 [==============================] - 61s 1s/step\nMCRMSE score:  (0.5403292025964797, [0.45582602561497576, 0.6248323795779834])\nScore loss:  {'mcrmse_score': 0.5403292025964797, 'Content_score': 0.45582602561497576, 'Wording_score': 0.6248323795779834}\nTraining Fold 1\n","output_type":"stream"},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nAll model checkpoint layers were used when initializing TFDebertaV2Model.\n\nAll the layers of TFDebertaV2Model were initialized from the model checkpoint at /kaggle/input/debertav3base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDebertaV2Model for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_ids (InputLayer)         [(None, 360)]        0           []                               \n                                                                                                  \n attention_mask (InputLayer)    [(None, 360)]        0           []                               \n                                                                                                  \n deberta (TFDebertaV2MainLayer)  TFBaseModelOutput(l  183831552  ['input_ids[0][0]',              \n                                ast_hidden_state=(N               'attention_mask[0][0]']         \n                                one, 360, 768),                                                   \n                                 hidden_states=((No                                               \n                                ne, 360, 768),                                                    \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768)),                                               \n                                 attentions=None)                                                 \n                                                                                                  \n global_average_pooling1d (Glob  (None, 768)         0           ['deberta[0][13]']               \n alAveragePooling1D)                                                                              \n                                                                                                  \n dense (Dense)                  (None, 2)            1538        ['global_average_pooling1d[0][0]'\n                                                                 ]                                \n                                                                                                  \n rescaling (Rescaling)          (None, 2)            0           ['dense[0][0]']                  \n                                                                                                  \n==================================================================================================\nTotal params: 183,833,090\nTrainable params: 183,833,090\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\nEpoch 1/10\n1344/1344 [==============================] - ETA: 0s - loss: 0.4919 - mse: 0.4919\nEpoch 1: val_loss improved from inf to 0.43607, saving model to /kaggle/working/models/model_fold_1.ckpt\n1344/1344 [==============================] - 1035s 719ms/step - loss: 0.4919 - mse: 0.4919 - val_loss: 0.4361 - val_mse: 0.4361 - lr: 2.0000e-05\nEpoch 2/10\n1344/1344 [==============================] - ETA: 0s - loss: 0.3370 - mse: 0.3370\nEpoch 2: val_loss improved from 0.43607 to 0.32748, saving model to /kaggle/working/models/model_fold_1.ckpt\n1344/1344 [==============================] - 941s 700ms/step - loss: 0.3370 - mse: 0.3370 - val_loss: 0.3275 - val_mse: 0.3275 - lr: 1.8000e-05\nEpoch 3/10\n1344/1344 [==============================] - ETA: 0s - loss: 0.2594 - mse: 0.2594\nEpoch 3: val_loss did not improve from 0.32748\n1344/1344 [==============================] - 923s 686ms/step - loss: 0.2594 - mse: 0.2594 - val_loss: 0.3512 - val_mse: 0.3512 - lr: 1.6200e-05\nEpoch 4/10\n1344/1344 [==============================] - ETA: 0s - loss: 0.1996 - mse: 0.1996\nEpoch 4: val_loss did not improve from 0.32748\nRestoring model weights from the end of the best epoch: 2.\n1344/1344 [==============================] - 913s 679ms/step - loss: 0.1996 - mse: 0.1996 - val_loss: 0.3975 - val_mse: 0.3975 - lr: 1.4580e-05\nEpoch 4: early stopping\n56/56 [==============================] - 65s 1s/step - loss: 0.3275 - mse: 0.3275\n56/56 [==============================] - 69s 1s/step\nMCRMSE score:  (0.5660625244794467, [0.48206980565181096, 0.6500552433070825])\nScore loss:  {'mcrmse_score': 0.5660625244794467, 'Content_score': 0.48206980565181096, 'Wording_score': 0.6500552433070825}\nTraining Fold 2\n","output_type":"stream"},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nAll model checkpoint layers were used when initializing TFDebertaV2Model.\n\nAll the layers of TFDebertaV2Model were initialized from the model checkpoint at /kaggle/input/debertav3base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDebertaV2Model for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_ids (InputLayer)         [(None, 360)]        0           []                               \n                                                                                                  \n attention_mask (InputLayer)    [(None, 360)]        0           []                               \n                                                                                                  \n deberta (TFDebertaV2MainLayer)  TFBaseModelOutput(l  183831552  ['input_ids[0][0]',              \n                                ast_hidden_state=(N               'attention_mask[0][0]']         \n                                one, 360, 768),                                                   \n                                 hidden_states=((No                                               \n                                ne, 360, 768),                                                    \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768)),                                               \n                                 attentions=None)                                                 \n                                                                                                  \n global_average_pooling1d (Glob  (None, 768)         0           ['deberta[0][13]']               \n alAveragePooling1D)                                                                              \n                                                                                                  \n dense (Dense)                  (None, 2)            1538        ['global_average_pooling1d[0][0]'\n                                                                 ]                                \n                                                                                                  \n rescaling (Rescaling)          (None, 2)            0           ['dense[0][0]']                  \n                                                                                                  \n==================================================================================================\nTotal params: 183,833,090\nTrainable params: 183,833,090\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\nEpoch 1/10\n1344/1344 [==============================] - ETA: 0s - loss: 0.4688 - mse: 0.4688\nEpoch 1: val_loss improved from inf to 0.44176, saving model to /kaggle/working/models/model_fold_2.ckpt\n1344/1344 [==============================] - 1029s 714ms/step - loss: 0.4688 - mse: 0.4688 - val_loss: 0.4418 - val_mse: 0.4418 - lr: 2.0000e-05\nEpoch 2/10\n1344/1344 [==============================] - ETA: 0s - loss: 0.3205 - mse: 0.3205\nEpoch 2: val_loss improved from 0.44176 to 0.32976, saving model to /kaggle/working/models/model_fold_2.ckpt\n1344/1344 [==============================] - 940s 699ms/step - loss: 0.3205 - mse: 0.3205 - val_loss: 0.3298 - val_mse: 0.3298 - lr: 1.8000e-05\nEpoch 3/10\n1344/1344 [==============================] - ETA: 0s - loss: 0.2365 - mse: 0.2365\nEpoch 3: val_loss improved from 0.32976 to 0.31048, saving model to /kaggle/working/models/model_fold_2.ckpt\n1344/1344 [==============================] - 942s 701ms/step - loss: 0.2365 - mse: 0.2365 - val_loss: 0.3105 - val_mse: 0.3105 - lr: 1.6200e-05\nEpoch 4/10\n1344/1344 [==============================] - ETA: 0s - loss: 0.1663 - mse: 0.1663\nEpoch 4: val_loss improved from 0.31048 to 0.29685, saving model to /kaggle/working/models/model_fold_2.ckpt\n1344/1344 [==============================] - 942s 701ms/step - loss: 0.1663 - mse: 0.1663 - val_loss: 0.2969 - val_mse: 0.2969 - lr: 1.4580e-05\nEpoch 5/10\n1344/1344 [==============================] - ETA: 0s - loss: 0.1142 - mse: 0.1142\nEpoch 5: val_loss did not improve from 0.29685\n1344/1344 [==============================] - 922s 686ms/step - loss: 0.1142 - mse: 0.1142 - val_loss: 0.3282 - val_mse: 0.3282 - lr: 1.3122e-05\nEpoch 6/10\n1344/1344 [==============================] - ETA: 0s - loss: 0.0827 - mse: 0.0827\nEpoch 6: val_loss did not improve from 0.29685\nRestoring model weights from the end of the best epoch: 4.\n1344/1344 [==============================] - 922s 686ms/step - loss: 0.0827 - mse: 0.0827 - val_loss: 0.3276 - val_mse: 0.3276 - lr: 1.1810e-05\nEpoch 6: early stopping\n56/56 [==============================] - 65s 1s/step - loss: 0.2969 - mse: 0.2969\n56/56 [==============================] - 69s 1s/step\nMCRMSE score:  (0.5392916953429092, [0.4617103377049243, 0.6168730529808941])\nScore loss:  {'mcrmse_score': 0.5392916953429092, 'Content_score': 0.4617103377049243, 'Wording_score': 0.6168730529808941}\nTraining Fold 3\n","output_type":"stream"},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nAll model checkpoint layers were used when initializing TFDebertaV2Model.\n\nAll the layers of TFDebertaV2Model were initialized from the model checkpoint at /kaggle/input/debertav3base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDebertaV2Model for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_ids (InputLayer)         [(None, 360)]        0           []                               \n                                                                                                  \n attention_mask (InputLayer)    [(None, 360)]        0           []                               \n                                                                                                  \n deberta (TFDebertaV2MainLayer)  TFBaseModelOutput(l  183831552  ['input_ids[0][0]',              \n                                ast_hidden_state=(N               'attention_mask[0][0]']         \n                                one, 360, 768),                                                   \n                                 hidden_states=((No                                               \n                                ne, 360, 768),                                                    \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768)),                                               \n                                 attentions=None)                                                 \n                                                                                                  \n global_average_pooling1d (Glob  (None, 768)         0           ['deberta[0][13]']               \n alAveragePooling1D)                                                                              \n                                                                                                  \n dense (Dense)                  (None, 2)            1538        ['global_average_pooling1d[0][0]'\n                                                                 ]                                \n                                                                                                  \n rescaling (Rescaling)          (None, 2)            0           ['dense[0][0]']                  \n                                                                                                  \n==================================================================================================\nTotal params: 183,833,090\nTrainable params: 183,833,090\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\nEpoch 1/10\n1344/1344 [==============================] - ETA: 0s - loss: 0.4729 - mse: 0.4729\nEpoch 1: val_loss improved from inf to 0.36127, saving model to /kaggle/working/models/model_fold_3.ckpt\n1344/1344 [==============================] - 1034s 718ms/step - loss: 0.4729 - mse: 0.4729 - val_loss: 0.3613 - val_mse: 0.3613 - lr: 2.0000e-05\nEpoch 2/10\n1344/1344 [==============================] - ETA: 0s - loss: 0.3153 - mse: 0.3153\nEpoch 2: val_loss improved from 0.36127 to 0.31607, saving model to /kaggle/working/models/model_fold_3.ckpt\n1344/1344 [==============================] - 932s 693ms/step - loss: 0.3153 - mse: 0.3153 - val_loss: 0.3161 - val_mse: 0.3161 - lr: 1.8000e-05\nEpoch 3/10\n1344/1344 [==============================] - ETA: 0s - loss: 0.2485 - mse: 0.2485\nEpoch 3: val_loss did not improve from 0.31607\n1344/1344 [==============================] - 928s 690ms/step - loss: 0.2485 - mse: 0.2485 - val_loss: 0.4896 - val_mse: 0.4896 - lr: 1.6200e-05\nEpoch 4/10\n1344/1344 [==============================] - ETA: 0s - loss: 0.1853 - mse: 0.1853\nEpoch 4: val_loss did not improve from 0.31607\nRestoring model weights from the end of the best epoch: 2.\n1344/1344 [==============================] - 927s 690ms/step - loss: 0.1853 - mse: 0.1853 - val_loss: 0.3422 - val_mse: 0.3422 - lr: 1.4580e-05\nEpoch 4: early stopping\n56/56 [==============================] - 65s 1s/step - loss: 0.3161 - mse: 0.3161\n56/56 [==============================] - 70s 1s/step\nMCRMSE score:  (0.5574268216513679, [0.4842934905256732, 0.6305601527770626])\nScore loss:  {'mcrmse_score': 0.5574268216513679, 'Content_score': 0.4842934905256732, 'Wording_score': 0.6305601527770626}\n","output_type":"stream"}]},{"cell_type":"code","source":"# preprocessing test data\nSummaries_test['text'] = Summaries_test['text'].apply(lambda x: clean_data(x))\nSummaries_test['prompt_question'] = Summaries_test['prompt_question'].apply(lambda x: clean_data(x))\nSummaries_test['prompt_title'] = Summaries_test['prompt_title'].apply(lambda x: clean_data(x))\nSummaries_test['prompt_text'] = Summaries_test['prompt_text'].apply(lambda x: clean_data(x))\n","metadata":{"execution":{"iopub.status.busy":"2023-08-19T16:54:11.342547Z","iopub.execute_input":"2023-08-19T16:54:11.342964Z","iopub.status.idle":"2023-08-19T16:54:11.354517Z","shell.execute_reply.started":"2023-08-19T16:54:11.342932Z","shell.execute_reply":"2023-08-19T16:54:11.353544Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"tokenizer = deberta_tokenizer(CONFIG.model_name)\ninput_ids_train, attention_masks_train = deberta_encode(Summaries_test[CONFIG.train_cols].values, tokenizer, CONFIG.max_length)\ninputs_trains = [input_ids_train, attention_masks_train]","metadata":{"execution":{"iopub.status.busy":"2023-08-19T16:55:15.221444Z","iopub.execute_input":"2023-08-19T16:55:15.221718Z","iopub.status.idle":"2023-08-19T16:55:16.414347Z","shell.execute_reply.started":"2023-08-19T16:55:15.221694Z","shell.execute_reply":"2023-08-19T16:55:16.413319Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"best_model = tf.keras.models.load_model('/kaggle/working/models/model_2_/kaggle/input/debertav3base')","metadata":{"execution":{"iopub.status.busy":"2023-08-19T16:52:00.325442Z","iopub.execute_input":"2023-08-19T16:52:00.325812Z","iopub.status.idle":"2023-08-19T16:52:37.014141Z","shell.execute_reply.started":"2023-08-19T16:52:00.325781Z","shell.execute_reply":"2023-08-19T16:52:37.012445Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"best_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T16:55:20.053069Z","iopub.execute_input":"2023-08-19T16:55:20.053456Z","iopub.status.idle":"2023-08-19T16:55:20.137139Z","shell.execute_reply.started":"2023-08-19T16:55:20.053413Z","shell.execute_reply":"2023-08-19T16:55:20.136109Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_ids (InputLayer)         [(None, 360)]        0           []                               \n                                                                                                  \n attention_mask (InputLayer)    [(None, 360)]        0           []                               \n                                                                                                  \n deberta (TFDebertaV2MainLayer)  {'last_hidden_state  183831552  ['input_ids[0][0]',              \n                                ': (None, 360, 768)               'attention_mask[0][0]']         \n                                , 'hidden_states':                                                \n                                ((None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768),                                                \n                                 (None, 360, 768))}                                               \n                                                                                                  \n global_average_pooling1d (Glob  (None, 768)         0           ['deberta[0][13]']               \n alAveragePooling1D)                                                                              \n                                                                                                  \n dense (Dense)                  (None, 2)            1538        ['global_average_pooling1d[0][0]'\n                                                                 ]                                \n                                                                                                  \n rescaling (Rescaling)          (None, 2)            0           ['dense[0][0]']                  \n                                                                                                  \n==================================================================================================\nTotal params: 183,833,090\nTrainable params: 183,833,090\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"y_test_pred = best_model.predict(inputs_trains)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T16:55:22.888969Z","iopub.execute_input":"2023-08-19T16:55:22.889356Z","iopub.status.idle":"2023-08-19T16:55:29.684405Z","shell.execute_reply.started":"2023-08-19T16:55:22.889326Z","shell.execute_reply":"2023-08-19T16:55:29.683376Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 4s 4s/step\n","output_type":"stream"}]},{"cell_type":"code","source":"valid_final_df = pd.DataFrame(y_test_pred, columns = ['content', 'wordings']) \nValid_final = pd.concat([Summaries_test['student_id'], valid_final_df], axis=1)\nValid_final.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T16:55:40.613582Z","iopub.execute_input":"2023-08-19T16:55:40.613975Z","iopub.status.idle":"2023-08-19T16:55:40.626267Z","shell.execute_reply.started":"2023-08-19T16:55:40.613945Z","shell.execute_reply":"2023-08-19T16:55:40.625074Z"},"trusted":true},"execution_count":22,"outputs":[]}]}