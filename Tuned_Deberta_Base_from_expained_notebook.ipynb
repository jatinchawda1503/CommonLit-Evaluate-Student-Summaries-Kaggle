{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daa0dabd",
   "metadata": {
    "papermill": {
     "duration": 0.007798,
     "end_time": "2023-10-09T17:58:11.013985",
     "exception": false,
     "start_time": "2023-10-09T17:58:11.006187",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Explained Tuned Debertav3+LGBM 📈\n",
    "\n",
    "## Introduction 🌟\n",
    "Welcome to this Jupyter notebook developed for the CommonLit - Evaluate Student Summaries\n",
    "! This notebook is designed to help you participate in the competition and make Automatically assess summaries written by students in grades 3-12.\n",
    "\n",
    "### Inspiration and Credits 🙌\n",
    "This notebook is inspired by the work of dangnguyen1997, available at [this Kaggle project](https://www.kaggle.com/code/dangnguyen97/tuned-debertav3-lgbm). i extend our gratitude to dangnguyen1997 for sharing their insights and code.\n",
    "\n",
    "🌟 Explore my profile and other public projects, and don't forget to share your feedback! \n",
    "👉 [Visit my Profile](https://www.kaggle.com/zulqarnainali) 👈\n",
    "\n",
    "🙏 Thank you for taking the time to review my work, and please give it a thumbs-up if you found it valuable! 👍\n",
    "\n",
    "## Purpose 🎯\n",
    "The primary purpose of this notebook is to:\n",
    "- Load and preprocess the competition data 📁\n",
    "- Engineer relevant features for model training 🏋️‍♂️\n",
    "- Train predictive models to make target variable predictions 🧠\n",
    "- Submit predictions to the competition environment 📤\n",
    "\n",
    "## Notebook Structure 📚\n",
    "This notebook is structured as follows:\n",
    "1. **Data Preparation**: In this section, we load and preprocess the competition data.\n",
    "2. **Feature Engineering**: We generate and select relevant features for model training.\n",
    "3. **Model Training**: We train machine learning models on the prepared data.\n",
    "4. **Prediction and Submission**: We make predictions on the test data and submit them for evaluation.\n",
    "5. **Conclusion**: We summarize the key findings and results.\n",
    "\n",
    "## How to Use 🛠️\n",
    "To use this notebook effectively, please follow these steps:\n",
    "1. Ensure you have the competition data and environment set up.\n",
    "2. Execute each cell sequentially to perform data preparation, feature engineering, model training, and prediction submission.\n",
    "3. Customize and adapt the code as needed to improve model performance or experiment with different approaches.\n",
    "\n",
    "**Note**: Make sure to replace any placeholder paths or configurations with your specific information.\n",
    "\n",
    "## Acknowledgments 🙏\n",
    "I acknowledge the OThe Learning Agency Lab organizers for providing the dataset and the competition platform.\n",
    "\n",
    "Let's get started! Feel free to reach out if you have any questions or need assistance along the way.\n",
    "👉 [Visit my Profile](https://www.kaggle.com/zulqarnainali) 👈"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8270ea3",
   "metadata": {
    "papermill": {
     "duration": 0.006933,
     "end_time": "2023-10-09T17:58:11.028269",
     "exception": false,
     "start_time": "2023-10-09T17:58:11.021336",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 📚 Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4f4bf6",
   "metadata": {
    "papermill": {
     "duration": 0.006865,
     "end_time": "2023-10-09T17:58:11.042225",
     "exception": false,
     "start_time": "2023-10-09T17:58:11.035360",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**:\n",
    "\n",
    "\n",
    ". `!pip install \"/kaggle/input/autocorrect/autocorrect-2.6.1.tar\"`:\n",
    "   - This line uses the `!` symbol to execute a shell command within a Jupyter Notebook or similar environment.\n",
    "   - It invokes the `pip install` command, which is a package manager for Python, used to install Python packages.\n",
    "   - The package it is trying to install is specified with the path \"/kaggle/input/autocorrect/autocorrect-2.6.1.tar\".\n",
    "   - The path points to a `.tar` file named \"autocorrect-2.6.1.tar\" located within the \"/kaggle/input/autocorrect/\" directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c28712f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-10-09T17:58:11.058167Z",
     "iopub.status.busy": "2023-10-09T17:58:11.057852Z",
     "iopub.status.idle": "2023-10-09T17:59:18.134503Z",
     "shell.execute_reply": "2023-10-09T17:59:18.133341Z"
    },
    "papermill": {
     "duration": 67.087585,
     "end_time": "2023-10-09T17:59:18.136909",
     "exception": false,
     "start_time": "2023-10-09T17:58:11.049324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/autocorrect/autocorrect-2.6.1.tar\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\r\n",
      "\u001b[?25hBuilding wheels for collected packages: autocorrect\r\n",
      "  Building wheel for autocorrect (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622363 sha256=6725acc788d33de97fa3fe0aac22ef9fe948e7d8a9e611bc1741c400b74ab57c\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/db/69/42/0fb0421d2fe70d195a04665edc760cfe5fd341d7bb8d8e0aaa\r\n",
      "Successfully built autocorrect\r\n",
      "Installing collected packages: autocorrect\r\n",
      "Successfully installed autocorrect-2.6.1\r\n",
      "Processing /kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\r\n",
      "Installing collected packages: pyspellchecker\r\n",
      "Successfully installed pyspellchecker-0.7.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install \"/kaggle/input/autocorrect/autocorrect-2.6.1.tar\"\n",
    "!pip install \"/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b288d40",
   "metadata": {
    "papermill": {
     "duration": 0.007389,
     "end_time": "2023-10-09T17:59:18.152434",
     "exception": false,
     "start_time": "2023-10-09T17:59:18.145045",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 📚 Importing other Libraries and Setting Environment 🔧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70e7d14a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T17:59:18.169781Z",
     "iopub.status.busy": "2023-10-09T17:59:18.169458Z",
     "iopub.status.idle": "2023-10-09T17:59:36.170003Z",
     "shell.execute_reply": "2023-10-09T17:59:36.169151Z"
    },
    "papermill": {
     "duration": 18.011936,
     "end_time": "2023-10-09T17:59:36.172192",
     "exception": false,
     "start_time": "2023-10-09T17:59:18.160256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from typing import List  # 👉 Importing the List type from the typing module\n",
    "import numpy as np  # 👉 Importing the NumPy library and aliasing it as np\n",
    "import pandas as pd  # 👉 Importing the Pandas library and aliasing it as pd\n",
    "import warnings  # 👉 Importing the warnings module\n",
    "import logging  # 👉 Importing the logging module\n",
    "import os  # 👉 Importing the os module\n",
    "import shutil  # 👉 Importing the shutil module\n",
    "import json  # 👉 Importing the json module\n",
    "import transformers  # 👉 Importing the transformers library\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification  # 👉 Importing specific classes from transformers\n",
    "from transformers import DataCollatorWithPadding  # 👉 Importing the DataCollatorWithPadding class from transformers\n",
    "from datasets import Dataset, load_dataset, load_from_disk  # 👉 Importing specific functions/classes from the datasets module\n",
    "from transformers import TrainingArguments, Trainer  # 👉 Importing specific classes from transformers\n",
    "from datasets import load_metric, disable_progress_bar  # 👉 Importing specific functions/classes from datasets\n",
    "from sklearn.metrics import mean_squared_error  # 👉 Importing the mean_squared_error function from scikit-learn\n",
    "import torch  # 👉 Importing the torch library\n",
    "from sklearn.model_selection import KFold, GroupKFold  # 👉 Importing KFold and GroupKFold classes from scikit-learn\n",
    "from tqdm import tqdm  # 👉 Importing the tqdm function from the tqdm module\n",
    "\n",
    "import nltk  # 👉 Importing the nltk library\n",
    "from nltk.corpus import stopwords  # 👉 Importing the stopwords corpus from nltk\n",
    "from nltk.tokenize import word_tokenize  # 👉 Importing the word_tokenize function from nltk\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer  # 👉 Importing the TreebankWordDetokenizer class from nltk\n",
    "from collections import Counter  # 👉 Importing the Counter class from the collections module\n",
    "import spacy  # 👉 Importing the spacy library\n",
    "import re  # 👉 Importing the re module\n",
    "from autocorrect import Speller  # 👉 Importing the Speller class from autocorrect\n",
    "from spellchecker import SpellChecker  # 👉 Importing the SpellChecker class from spellchecker\n",
    "import lightgbm as lgb  # 👉 Importing the lightgbm library and aliasing it as lgb\n",
    "\n",
    "warnings.simplefilter(\"ignore\")  # 👉 Ignore warnings in the code\n",
    "logging.disable(logging.ERROR)  # 👉 Disable logging to ERROR level\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # 👉 Set an environment variable for tokenizers\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 👉 Set the minimum Tensorflow log level to 3\n",
    "disable_progress_bar()  # 👉 Disable progress bars\n",
    "tqdm.pandas()  # 👉 Enable tqdm progress bars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29243b95",
   "metadata": {
    "papermill": {
     "duration": 0.007354,
     "end_time": "2023-10-09T17:59:36.187550",
     "exception": false,
     "start_time": "2023-10-09T17:59:36.180196",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Seed Initialization for Reproducibility 🌱"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d3f0d8",
   "metadata": {
    "papermill": {
     "duration": 0.007582,
     "end_time": "2023-10-09T17:59:36.202737",
     "exception": false,
     "start_time": "2023-10-09T17:59:36.195155",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**:\n",
    "\n",
    "\n",
    "1. `def seed_everything(seed: int):`\n",
    "   - Defines a function named `seed_everything` that takes an integer argument `seed`. This function will initialize seeds for various random number generators.\n",
    "\n",
    "2. `import random, os`\n",
    "   - Imports the `random` and `os` modules, which are used for random number generation and system-related operations, respectively.\n",
    "\n",
    "3. `import numpy as np`\n",
    "   - Imports the `numpy` library and aliases it as `np`. NumPy is commonly used for numerical computations.\n",
    "\n",
    "4. `import torch`\n",
    "   - Imports the `torch` library, which is the core library for PyTorch, a popular deep learning framework.\n",
    "\n",
    "5. `random.seed(seed)`\n",
    "   - Sets the seed for Python's random module to the provided `seed` value. This ensures reproducibility of random numbers generated using Python's `random` functions.\n",
    "\n",
    "6. `os.environ['PYTHONHASHSEED'] = str(seed)`\n",
    "   - Sets the hash seed for Python to the provided `seed` value. This helps ensure consistent hash-based operations across different runs of the program.\n",
    "\n",
    "7. `np.random.seed(seed)`\n",
    "   - Sets the seed for NumPy's random number generator to the provided `seed`, making NumPy's random operations reproducible.\n",
    "\n",
    "8. `torch.manual_seed(seed)`\n",
    "   - Sets the seed for PyTorch's random number generator for CPU operations to the provided `seed`.\n",
    "\n",
    "9. `torch.cuda.manual_seed(seed)`\n",
    "   - Sets the seed for PyTorch's random number generator for GPU operations to the provided `seed`.\n",
    "\n",
    "10. `torch.backends.cudnn.deterministic = True`\n",
    "    - Enforces deterministic behavior for CuDNN (CUDA Deep Neural Network library) to ensure consistent results when using GPU acceleration.\n",
    "\n",
    "11. `torch.backends.cudnn.benchmark = True`\n",
    "    - Enables CuDNN benchmark mode, which can optimize GPU performance during training.\n",
    "\n",
    "12. `seed_everything(seed=42)`\n",
    "    - Calls the `seed_everything` function with a specific seed value, in this case, `42`, to initialize all the random number generators with this seed value, ensuring reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e3aca93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T17:59:36.219238Z",
     "iopub.status.busy": "2023-10-09T17:59:36.218779Z",
     "iopub.status.idle": "2023-10-09T17:59:36.227890Z",
     "shell.execute_reply": "2023-10-09T17:59:36.227146Z"
    },
    "papermill": {
     "duration": 0.019266,
     "end_time": "2023-10-09T17:59:36.229521",
     "exception": false,
     "start_time": "2023-10-09T17:59:36.210255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca28eb8",
   "metadata": {
    "papermill": {
     "duration": 0.007364,
     "end_time": "2023-10-09T17:59:36.244472",
     "exception": false,
     "start_time": "2023-10-09T17:59:36.237108",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configuration Settings 🛠️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5aa227",
   "metadata": {
    "papermill": {
     "duration": 0.007265,
     "end_time": "2023-10-09T17:59:36.259248",
     "exception": false,
     "start_time": "2023-10-09T17:59:36.251983",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**:\n",
    "\n",
    "\n",
    "1. `class CFG:`: \n",
    "   - Defines a Python class named `CFG` for configuration settings. This class will hold various configuration parameters for your project.\n",
    "\n",
    "2. `model_name = \"debertav3base\"`:\n",
    "   - Defines a variable `model_name` with the value \"debertav3base\" as a reference to the model being used.\n",
    "\n",
    "3. `learning_rate = 0.000016`:\n",
    "   - Sets the learning rate (a hyperparameter for gradient descent) to `0.000016`.\n",
    "\n",
    "4. `weight_decay = 0.03`:\n",
    "   - Sets the weight decay, a regularization term (L2 regularization), to `0.03`. It helps prevent overfitting during training.\n",
    "\n",
    "5. `hidden_dropout_prob = 0.007`:\n",
    "   - Defines the dropout probability for hidden layers in the model, set to `0.007`. Dropout helps prevent overfitting by randomly dropping some neurons during training.\n",
    "\n",
    "6. `attention_probs_dropout_prob = 0.007`:\n",
    "   - Sets the dropout probability for attention layers in the model to `0.007`. This dropout is specific to the attention mechanism.\n",
    "\n",
    "7. `num_train_epochs = 5`:\n",
    "   - Specifies the number of training epochs (iterations over the entire training dataset) as `5`.\n",
    "\n",
    "8. `n_splits = 4`:\n",
    "   - Defines the number of splits for cross-validation, typically used to assess model performance. Set to `4` in this case.\n",
    "\n",
    "9. `batch_size = 12`:\n",
    "   - Sets the batch size for training data to `12`. This determines how many samples are processed together in each training step.\n",
    "\n",
    "10. `random_seed = 42`:\n",
    "    - Sets the random seed to `42` for reproducibility. This ensures that random operations produce the same results across different runs.\n",
    "\n",
    "11. `save_steps = 100`:\n",
    "    - Specifies the number of steps (or iterations) before saving model checkpoints during training. In this case, checkpoints are saved every `100` steps.\n",
    "\n",
    "12. `max_length = 512`:\n",
    "    - Defines the maximum sequence length for input data as `512`. This can be important when working with text data, as it limits the length of input sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b356d8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T17:59:36.275349Z",
     "iopub.status.busy": "2023-10-09T17:59:36.275151Z",
     "iopub.status.idle": "2023-10-09T17:59:36.279843Z",
     "shell.execute_reply": "2023-10-09T17:59:36.278978Z"
    },
    "papermill": {
     "duration": 0.014695,
     "end_time": "2023-10-09T17:59:36.281456",
     "exception": false,
     "start_time": "2023-10-09T17:59:36.266761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a class for configuration settings\n",
    "class CFG:\n",
    "    model_name = \"debertav3base\"  # Model name for reference\n",
    "    learning_rate = 0.000016 # Learning rate 📚\n",
    "    weight_decay = 0.03  # Weight decay (L2 regularization) 🏋️‍♂️\n",
    "    hidden_dropout_prob = 0.007  # Dropout probability for hidden layers 🙈\n",
    "    attention_probs_dropout_prob = 0.007  # Dropout probability for attention layers 🙉\n",
    "    num_train_epochs = 5  # Number of training epochs 🚂\n",
    "    n_splits = 4  # Number of splits for cross-validation 🔄\n",
    "    batch_size = 12  # Batch size for training data 📦\n",
    "    random_seed = 42  # Random seed for reproducibility 🌱\n",
    "    save_steps = 100  # Number of steps before saving model checkpoints 📥\n",
    "    max_length = 512  # Maximum sequence length for input data 📏\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ad7ab4",
   "metadata": {
    "papermill": {
     "duration": 0.007497,
     "end_time": "2023-10-09T17:59:36.296466",
     "exception": false,
     "start_time": "2023-10-09T17:59:36.288969",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##  Data Loading 📁"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86303c0d",
   "metadata": {
    "papermill": {
     "duration": 0.007679,
     "end_time": "2023-10-09T17:59:36.355208",
     "exception": false,
     "start_time": "2023-10-09T17:59:36.347529",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**:\n",
    "\n",
    "1. `DATA_DIR = \"/kaggle/input/commonlit-evaluate-student-summaries/\"`:\n",
    "   - Defines a variable `DATA_DIR` to store the directory path where the data files are located.\n",
    "\n",
    "2. `prompts_train = pd.read_csv(DATA_DIR + \"prompts_train.csv\")`:\n",
    "   - Loads the training prompts data from the CSV file named \"prompts_train.csv\" located in `DATA_DIR` into a Pandas DataFrame named `prompts_train`.\n",
    "\n",
    "3. `prompts_test = pd.read_csv(DATA_DIR + \"prompts_test.csv\")`:\n",
    "   - Loads the testing prompts data from the CSV file named \"prompts_test.csv\" located in `DATA_DIR` into a Pandas DataFrame named `prompts_test`.\n",
    "\n",
    "4. `summaries_train = pd.read_csv(DATA_DIR + \"summaries_train.csv\")`:\n",
    "   - Loads the training summaries data from the CSV file named \"summaries_train.csv\" located in `DATA_DIR` into a Pandas DataFrame named `summaries_train`.\n",
    "\n",
    "5. `summaries_test = pd.read_csv(DATA_DIR + \"summaries_test.csv\")`:\n",
    "   - Loads the testing summaries data from the CSV file named \"summaries_test.csv\" located in `DATA_DIR` into a Pandas DataFrame named `summaries_test`.\n",
    "\n",
    "6. `sample_submission = pd.read_csv(DATA_DIR + \"sample_submission.csv\")`:\n",
    "   - Loads the sample submission data from the CSV file named \"sample_submission.csv\" located in `DATA_DIR` into a Pandas DataFrame named `sample_submission`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc173cd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T17:59:36.372530Z",
     "iopub.status.busy": "2023-10-09T17:59:36.372305Z",
     "iopub.status.idle": "2023-10-09T17:59:36.493439Z",
     "shell.execute_reply": "2023-10-09T17:59:36.492660Z"
    },
    "papermill": {
     "duration": 0.132241,
     "end_time": "2023-10-09T17:59:36.495688",
     "exception": false,
     "start_time": "2023-10-09T17:59:36.363447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the directory where the data files are located\n",
    "DATA_DIR = \"/kaggle/input/commonlit-evaluate-student-summaries/\"\n",
    "\n",
    "# Load the training prompts data from a CSV file\n",
    "prompts_train = pd.read_csv(DATA_DIR + \"prompts_train.csv\")\n",
    "\n",
    "# Load the test prompts data from a CSV file\n",
    "prompts_test = pd.read_csv(DATA_DIR + \"prompts_test.csv\")\n",
    "\n",
    "# Load the training summaries data from a CSV file\n",
    "summaries_train = pd.read_csv(DATA_DIR + \"summaries_train.csv\")\n",
    "\n",
    "# Load the test summaries data from a CSV file\n",
    "summaries_test = pd.read_csv(DATA_DIR + \"summaries_test.csv\")\n",
    "\n",
    "# Load the sample submission data from a CSV file\n",
    "sample_submission = pd.read_csv(DATA_DIR + \"sample_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f121c95",
   "metadata": {
    "papermill": {
     "duration": 0.00758,
     "end_time": "2023-10-09T17:59:36.511528",
     "exception": false,
     "start_time": "2023-10-09T17:59:36.503948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 📝 Text Preprocessing and Feature Extraction 🔍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d89c58",
   "metadata": {
    "papermill": {
     "duration": 0.007503,
     "end_time": "2023-10-09T17:59:36.526747",
     "exception": false,
     "start_time": "2023-10-09T17:59:36.519244",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**:\n",
    "\n",
    "\n",
    "1. `class Preprocessor:`:\n",
    "   - Defines a Python class named `Preprocessor` for text preprocessing and feature extraction.\n",
    "\n",
    "2. `def __init__(self, model_name: str) -> None:`:\n",
    "   - Defines the class constructor method, which initializes the class instance.\n",
    "   - Takes `model_name` as a parameter, which specifies the name of the model to be used for tokenization.\n",
    "\n",
    "3. `self.tokenizer = AutoTokenizer.from_pretrained(f\"/kaggle/input/{model_name\n",
    "\n",
    "}\")`:\n",
    "   - Initializes the tokenizer using the Hugging Face Transformers library based on the specified model name.\n",
    "\n",
    "4. `self.twd = TreebankWordDetokenizer()`:\n",
    "   - Initializes the `TreebankWordDetokenizer` object, which is used for detokenization.\n",
    "\n",
    "5. `self.STOP_WORDS = set(stopwords.words('english'))`:\n",
    "   - Defines a set of English stopwords using NLTK's `stopwords` corpus.\n",
    "\n",
    "6. `self.spacy_ner_model = spacy.load('en_core_web_sm',)`:\n",
    "   - Loads the spaCy Named Entity Recognition (NER) model for English.\n",
    "\n",
    "7. `self.speller = Speller(lang='en')`:\n",
    "   - Initializes the `Speller` object from the `pyspellchecker` library for spelling correction.\n",
    "\n",
    "8. `self.spellchecker = SpellChecker()`:\n",
    "   - Initializes the `SpellChecker` object from the `spellchecker` library for spelling checking.\n",
    "\n",
    "The rest of the code defines various methods within the `Preprocessor` class for performing text preprocessing and feature extraction tasks, including word overlap count, n-grams, NER overlap count, quotes count, spelling correction, and more. These methods are used to process the input data and extract relevant features.\n",
    "\n",
    "At the end of the cell, an instance of the `Preprocessor` class named `preprocessor` is created, and it is configured with the specified `model_name` from the configuration settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37c2398c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T17:59:36.544484Z",
     "iopub.status.busy": "2023-10-09T17:59:36.543953Z",
     "iopub.status.idle": "2023-10-09T17:59:38.333177Z",
     "shell.execute_reply": "2023-10-09T17:59:38.332282Z"
    },
    "papermill": {
     "duration": 1.80075,
     "end_time": "2023-10-09T17:59:38.335311",
     "exception": false,
     "start_time": "2023-10-09T17:59:36.534561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a class for text preprocessing and feature extraction\n",
    "class Preprocessor:\n",
    "    def __init__(self, model_name: str) -> None:\n",
    "        # Initialize the tokenizer for the specified model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(f\"/kaggle/input/{model_name}\")\n",
    "        \n",
    "        # Initialize the TreebankWordDetokenizer for detokenization\n",
    "        self.twd = TreebankWordDetokenizer()\n",
    "        \n",
    "        # Define a set of English stopwords\n",
    "        self.STOP_WORDS = set(stopwords.words('english'))\n",
    "        \n",
    "        # Load the spaCy NER model for English\n",
    "        self.spacy_ner_model = spacy.load('en_core_web_sm',)\n",
    "        \n",
    "        # Initialize the pyspellchecker Speller for spelling correction\n",
    "        self.speller = Speller(lang='en')\n",
    "        \n",
    "        # Initialize the pyspellchecker SpellChecker for spelling checking\n",
    "        self.spellchecker = SpellChecker() \n",
    "        \n",
    "    def word_overlap_count(self, row):\n",
    "        \"\"\" Count the overlapping words between prompt and summary \"\"\"        \n",
    "        def check_is_stop_word(word):\n",
    "            return word in self.STOP_WORDS\n",
    "        \n",
    "        prompt_words = row['prompt_tokens']\n",
    "        summary_words = row['summary_tokens']\n",
    "        \n",
    "        # Filter out stopwords if they are defined\n",
    "        if self.STOP_WORDS:\n",
    "            prompt_words = list(filter(check_is_stop_word, prompt_words))\n",
    "            summary_words = list(filter(check_is_stop_word, summary_words))\n",
    "        \n",
    "        # Calculate the count of overlapping words\n",
    "        return len(set(prompt_words).intersection(set(summary_words)))\n",
    "            \n",
    "    def ngrams(self, token, n):\n",
    "        # Use the zip function to generate n-grams\n",
    "        ngrams = zip(*[token[i:] for i in range(n)])\n",
    "        return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "    def ngram_co_occurrence(self, row, n: int) -> int:\n",
    "        # Tokenize the original text and summary into words\n",
    "        original_tokens = row['prompt_tokens']\n",
    "        summary_tokens = row['summary_tokens']\n",
    "\n",
    "        # Generate n-grams for the original text and summary\n",
    "        original_ngrams = set(self.ngrams(original_tokens, n))\n",
    "        summary_ngrams = set(self.ngrams(summary_tokens, n))\n",
    "\n",
    "        # Calculate the number of common n-grams\n",
    "        common_ngrams = original_ngrams.intersection(summary_ngrams)\n",
    "        return len(common_ngrams)\n",
    "    \n",
    "    def ner_overlap_count(self, row, mode:str):\n",
    "        model = self.spacy_ner_model\n",
    "        def clean_ners(ner_list):\n",
    "            return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n",
    "        prompt = model(row['prompt_text'])\n",
    "        summary = model(row['text'])\n",
    "\n",
    "        if \"spacy\" in str(model):\n",
    "            prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n",
    "            summary_ner = set([(token.text, token.label_) for token in summary.ents])\n",
    "        elif \"stanza\" in str(model):\n",
    "            prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n",
    "            summary_ner = set([(token.text, token.type) for token in summary.ents])\n",
    "        else:\n",
    "            raise Exception(\"Model not supported\")\n",
    "\n",
    "        prompt_ner = clean_ners(prompt_ner)\n",
    "        summary_ner = clean_ners(summary_ner)\n",
    "\n",
    "        intersecting_ners = prompt_ner.intersection(summary_ner)\n",
    "        \n",
    "        ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            return ner_dict\n",
    "        elif mode == \"test\":\n",
    "            return {key: ner_dict.get(key) for key in self.ner_keys}\n",
    "\n",
    "    \n",
    "    def quotes_count(self, row):\n",
    "        summary = row['text']\n",
    "        text = row['prompt_text']\n",
    "        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n",
    "        if len(quotes_from_summary) > 0:\n",
    "            return [quote in text for quote in quotes_from_summary].count(True)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def spelling(self, text):\n",
    "        wordlist = text.split()\n",
    "        amount_miss = len(list(self.spellchecker.unknown(wordlist)))\n",
    "        return amount_miss\n",
    "    \n",
    "    def add_spelling_dictionary(self, tokens: List[str]) -> List[str]:\n",
    "        \"\"\"Dictionary update for pyspell checker and autocorrect\"\"\"\n",
    "        self.spellchecker.word_frequency.load_words(tokens)\n",
    "        self.speller.nlp_data.update({token: 1000 for token in tokens})\n",
    "    \n",
    "    def run(self, prompts: pd.DataFrame, summaries: pd.DataFrame, mode: str) -> pd.DataFrame:\n",
    "        # Before merge preprocessing\n",
    "        prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(lambda x: len(word_tokenize(x)))\n",
    "        prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "        summaries[\"summary_length\"] = summaries[\"text\"].apply(lambda x: len(word_tokenize(x)))\n",
    "        summaries[\"summary_tokens\"] = summaries[\"text\"].apply(lambda x: word_tokenize(x))\n",
    "        \n",
    "        # Add prompt tokens into spelling checker dictionary\n",
    "        prompts[\"prompt_tokens\"].apply(lambda x: self.add_spelling_dictionary(x))\n",
    "        \n",
    "        # Fix misspelling in summaries\n",
    "        summaries[\"fixed_summary_text\"] = summaries[\"text\"].progress_apply(lambda x: self.speller(x))\n",
    "        \n",
    "        # Count misspellings in summaries\n",
    "        summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(self.spelling)\n",
    "        \n",
    "        # Merge prompts and summaries\n",
    "        input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n",
    "\n",
    "        # After merge preprocessing\n",
    "        input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)\n",
    "        input_df['bigram_overlap_count'] = input_df.progress_apply(self.ngram_co_occurrence, args=(2,), axis=1)\n",
    "        input_df['bigram_overlap_ratio'] = input_df['bigram_overlap_count'] / (input_df['summary_length'] - 1)\n",
    "        input_df['trigram_overlap_count'] = input_df.progress_apply(self.ngram_co_occurrence, args=(3,), axis=1)\n",
    "        input_df['trigram_overlap_ratio'] = input_df['trigram_overlap_count'] / (input_df['summary_length'] - 2)\n",
    "        input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n",
    "        \n",
    "        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])\n",
    "\n",
    "# Create an instance of the Preprocessor class\n",
    "preprocessor = Preprocessor(model_name=CFG.model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ed0615",
   "metadata": {
    "papermill": {
     "duration": 0.007743,
     "end_time": "2023-10-09T17:59:38.351865",
     "exception": false,
     "start_time": "2023-10-09T17:59:38.344122",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 📝Text Preprocessing and Feature Extraction  🔍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dce54d",
   "metadata": {
    "papermill": {
     "duration": 0.00749,
     "end_time": "2023-10-09T17:59:38.367231",
     "exception": false,
     "start_time": "2023-10-09T17:59:38.359741",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**:\n",
    "\n",
    "1. `train = preprocessor.run(prompts_train, summaries_train, mode=\"train\")`:\n",
    "   - Calls the `run` method of the `preprocessor` instance to preprocess and extract features from the training data.\n",
    "   - The `mode` parameter is set to \"train\" to indicate that this is the training data.\n",
    "   - The resulting DataFrame is assigned to the variable `train`.\n",
    "\n",
    "2. `test = preprocessor.run(prompts_test, summaries_test, mode=\"test\")`:\n",
    "   - Calls the `run` method of the `preprocessor` instance to preprocess and extract features from the testing data.\n",
    "   - The `mode` parameter is set to \"test\" to indicate that this is the testing data.\n",
    "   - The resulting DataFrame is assigned to the variable `test`.\n",
    "\n",
    "3. `train.head()`:\n",
    "   - Displays the first few rows of the `train` DataFrame to provide an overview of the preprocessed training data.\n",
    "\n",
    "These lines of code preprocess and extract features from both the training and testing data, making them ready for use in machine learning or deep learning models. The `train` DataFrame contains the preprocessed training data, and the `test` DataFrame contains the preprocessed testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bc294df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T17:59:38.384274Z",
     "iopub.status.busy": "2023-10-09T17:59:38.383797Z",
     "iopub.status.idle": "2023-10-09T18:06:05.115179Z",
     "shell.execute_reply": "2023-10-09T18:06:05.114372Z"
    },
    "papermill": {
     "duration": 386.741944,
     "end_time": "2023-10-09T18:06:05.116929",
     "exception": false,
     "start_time": "2023-10-09T17:59:38.374985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7165/7165 [06:15<00:00, 19.10it/s]\n",
      "100%|██████████| 7165/7165 [00:00<00:00, 8706.39it/s]\n",
      "100%|██████████| 7165/7165 [00:00<00:00, 9758.80it/s]\n",
      "100%|██████████| 7165/7165 [00:01<00:00, 5245.91it/s]\n",
      "100%|██████████| 7165/7165 [00:01<00:00, 4583.73it/s]\n",
      "100%|██████████| 7165/7165 [00:00<00:00, 94170.59it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 4272.27it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 6817.24it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 3754.13it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 4109.04it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 3862.16it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 4300.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>fixed_summary_text</th>\n",
       "      <th>splling_err_num</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>word_overlap_count</th>\n",
       "      <th>bigram_overlap_count</th>\n",
       "      <th>bigram_overlap_ratio</th>\n",
       "      <th>trigram_overlap_count</th>\n",
       "      <th>trigram_overlap_ratio</th>\n",
       "      <th>quotes_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000e8c3c7ddb</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The third wave was an experimentto see how peo...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "      <td>64</td>\n",
       "      <td>The third wave was an experimental see how peo...</td>\n",
       "      <td>5</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
       "      <td>660</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>0.063492</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0020ae56ffbf</td>\n",
       "      <td>ebad26</td>\n",
       "      <td>They would rub it up with soda to make the sme...</td>\n",
       "      <td>-0.548304</td>\n",
       "      <td>0.506755</td>\n",
       "      <td>54</td>\n",
       "      <td>They would rub it up with soda to make the sme...</td>\n",
       "      <td>2</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>1076</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>10</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>004e978e639e</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>In Egypt, there were many occupations and soci...</td>\n",
       "      <td>3.128928</td>\n",
       "      <td>4.231226</td>\n",
       "      <td>269</td>\n",
       "      <td>In Egypt, there were many occupations and soci...</td>\n",
       "      <td>32</td>\n",
       "      <td>In complete sentences, summarize the structure...</td>\n",
       "      <td>Egyptian Social Structure</td>\n",
       "      <td>Egyptian society was structured like a pyramid...</td>\n",
       "      <td>625</td>\n",
       "      <td>22</td>\n",
       "      <td>52</td>\n",
       "      <td>0.194030</td>\n",
       "      <td>23</td>\n",
       "      <td>0.086142</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>005ab0199905</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>The highest class was Pharaohs these people we...</td>\n",
       "      <td>-0.210614</td>\n",
       "      <td>-0.471415</td>\n",
       "      <td>28</td>\n",
       "      <td>The highest class was Pharaohs these people we...</td>\n",
       "      <td>5</td>\n",
       "      <td>In complete sentences, summarize the structure...</td>\n",
       "      <td>Egyptian Social Structure</td>\n",
       "      <td>Egyptian society was structured like a pyramid...</td>\n",
       "      <td>625</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>5</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0070c9e7af47</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The Third Wave developed  rapidly because the ...</td>\n",
       "      <td>3.272894</td>\n",
       "      <td>3.219757</td>\n",
       "      <td>232</td>\n",
       "      <td>The Third Wave developed  rapidly because the ...</td>\n",
       "      <td>29</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
       "      <td>660</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>0.116883</td>\n",
       "      <td>5</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id prompt_id                                               text  \\\n",
       "0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n",
       "1  0020ae56ffbf    ebad26  They would rub it up with soda to make the sme...   \n",
       "2  004e978e639e    3b9047  In Egypt, there were many occupations and soci...   \n",
       "3  005ab0199905    3b9047  The highest class was Pharaohs these people we...   \n",
       "4  0070c9e7af47    814d6b  The Third Wave developed  rapidly because the ...   \n",
       "\n",
       "    content   wording  summary_length  \\\n",
       "0  0.205683  0.380538              64   \n",
       "1 -0.548304  0.506755              54   \n",
       "2  3.128928  4.231226             269   \n",
       "3 -0.210614 -0.471415              28   \n",
       "4  3.272894  3.219757             232   \n",
       "\n",
       "                                  fixed_summary_text  splling_err_num  \\\n",
       "0  The third wave was an experimental see how peo...                5   \n",
       "1  They would rub it up with soda to make the sme...                2   \n",
       "2  In Egypt, there were many occupations and soci...               32   \n",
       "3  The highest class was Pharaohs these people we...                5   \n",
       "4  The Third Wave developed  rapidly because the ...               29   \n",
       "\n",
       "                                     prompt_question  \\\n",
       "0  Summarize how the Third Wave developed over su...   \n",
       "1  Summarize the various ways the factory would u...   \n",
       "2  In complete sentences, summarize the structure...   \n",
       "3  In complete sentences, summarize the structure...   \n",
       "4  Summarize how the Third Wave developed over su...   \n",
       "\n",
       "                prompt_title  \\\n",
       "0             The Third Wave   \n",
       "1    Excerpt from The Jungle   \n",
       "2  Egyptian Social Structure   \n",
       "3  Egyptian Social Structure   \n",
       "4             The Third Wave   \n",
       "\n",
       "                                         prompt_text  prompt_length  \\\n",
       "0  Background \\r\\nThe Third Wave experiment took ...            660   \n",
       "1  With one member trimming beef in a cannery, an...           1076   \n",
       "2  Egyptian society was structured like a pyramid...            625   \n",
       "3  Egyptian society was structured like a pyramid...            625   \n",
       "4  Background \\r\\nThe Third Wave experiment took ...            660   \n",
       "\n",
       "   word_overlap_count  bigram_overlap_count  bigram_overlap_ratio  \\\n",
       "0                  14                     4              0.063492   \n",
       "1                  18                    22              0.415094   \n",
       "2                  22                    52              0.194030   \n",
       "3                   6                     6              0.222222   \n",
       "4                  23                    27              0.116883   \n",
       "\n",
       "   trigram_overlap_count  trigram_overlap_ratio  quotes_count  \n",
       "0                      0               0.000000             0  \n",
       "1                     10               0.192308             0  \n",
       "2                     23               0.086142             2  \n",
       "3                      5               0.192308             0  \n",
       "4                      5               0.021739             4  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform text preprocessing and feature extraction on training data\n",
    "train = preprocessor.run(prompts_train, summaries_train, mode=\"train\")\n",
    "\n",
    "# Perform text preprocessing and feature extraction on testing data\n",
    "test = preprocessor.run(prompts_test, summaries_test, mode=\"test\")\n",
    "\n",
    "# Display the first few rows of the training data\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eb4ca2",
   "metadata": {
    "papermill": {
     "duration": 0.113858,
     "end_time": "2023-10-09T18:06:05.304694",
     "exception": false,
     "start_time": "2023-10-09T18:06:05.190836",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Group K-Fold Cross-Validation Splitting 🔄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7076205",
   "metadata": {
    "papermill": {
     "duration": 0.074967,
     "end_time": "2023-10-09T18:06:05.452276",
     "exception": false,
     "start_time": "2023-10-09T18:06:05.377309",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**:\n",
    "\n",
    "1. `gkf = GroupKFold(n_splits=CFG.n_splits)`:\n",
    "   - Creates a `GroupKFold` object with the specified number of splits, which is determined by the `CFG.n_splits` configuration parameter.\n",
    "\n",
    "2. `for i, (_, val_index) in enumerate(gkf.split(train, groups=train[\"prompt_id\"])):`:\n",
    "   - Iterates through the cross-validation splits generated by `GroupKFold`.\n",
    "   - `i` is used as the fold index, and `_` is used to ignore the training indices.\n",
    "   - `val_index` contains the indices of the validation data for the current fold.\n",
    "\n",
    "3. `train.loc[val_index, \"fold\"] = i`:\n",
    "   - Assigns the fold index `i` to the rows in the `train` DataFrame that correspond to the validation data for the current fold.\n",
    "   - This allows for grouping and tracking data within each fold for cross-validation.\n",
    "\n",
    "4. `train.head()`:\n",
    "   - Displays the first few rows of the training data with the newly assigned fold numbers.\n",
    "   - Each row in the `train` DataFrame now has a \"fold\" column indicating which fold it belongs to for cross-validation.\n",
    "\n",
    "This code cell performs group-based K-Fold cross-validation splitting and assigns fold numbers to the training data. It's a common technique for evaluating machine learning models on multiple subsets of the data while ensuring that related data points stay within the same fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f407eda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T18:06:05.655689Z",
     "iopub.status.busy": "2023-10-09T18:06:05.655434Z",
     "iopub.status.idle": "2023-10-09T18:06:05.679246Z",
     "shell.execute_reply": "2023-10-09T18:06:05.678410Z"
    },
    "papermill": {
     "duration": 0.099823,
     "end_time": "2023-10-09T18:06:05.680938",
     "exception": false,
     "start_time": "2023-10-09T18:06:05.581115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>fixed_summary_text</th>\n",
       "      <th>splling_err_num</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>word_overlap_count</th>\n",
       "      <th>bigram_overlap_count</th>\n",
       "      <th>bigram_overlap_ratio</th>\n",
       "      <th>trigram_overlap_count</th>\n",
       "      <th>trigram_overlap_ratio</th>\n",
       "      <th>quotes_count</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000e8c3c7ddb</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The third wave was an experimentto see how peo...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "      <td>64</td>\n",
       "      <td>The third wave was an experimental see how peo...</td>\n",
       "      <td>5</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
       "      <td>660</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>0.063492</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0020ae56ffbf</td>\n",
       "      <td>ebad26</td>\n",
       "      <td>They would rub it up with soda to make the sme...</td>\n",
       "      <td>-0.548304</td>\n",
       "      <td>0.506755</td>\n",
       "      <td>54</td>\n",
       "      <td>They would rub it up with soda to make the sme...</td>\n",
       "      <td>2</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>1076</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>10</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>004e978e639e</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>In Egypt, there were many occupations and soci...</td>\n",
       "      <td>3.128928</td>\n",
       "      <td>4.231226</td>\n",
       "      <td>269</td>\n",
       "      <td>In Egypt, there were many occupations and soci...</td>\n",
       "      <td>32</td>\n",
       "      <td>In complete sentences, summarize the structure...</td>\n",
       "      <td>Egyptian Social Structure</td>\n",
       "      <td>Egyptian society was structured like a pyramid...</td>\n",
       "      <td>625</td>\n",
       "      <td>22</td>\n",
       "      <td>52</td>\n",
       "      <td>0.194030</td>\n",
       "      <td>23</td>\n",
       "      <td>0.086142</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>005ab0199905</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>The highest class was Pharaohs these people we...</td>\n",
       "      <td>-0.210614</td>\n",
       "      <td>-0.471415</td>\n",
       "      <td>28</td>\n",
       "      <td>The highest class was Pharaohs these people we...</td>\n",
       "      <td>5</td>\n",
       "      <td>In complete sentences, summarize the structure...</td>\n",
       "      <td>Egyptian Social Structure</td>\n",
       "      <td>Egyptian society was structured like a pyramid...</td>\n",
       "      <td>625</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>5</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0070c9e7af47</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The Third Wave developed  rapidly because the ...</td>\n",
       "      <td>3.272894</td>\n",
       "      <td>3.219757</td>\n",
       "      <td>232</td>\n",
       "      <td>The Third Wave developed  rapidly because the ...</td>\n",
       "      <td>29</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
       "      <td>660</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>0.116883</td>\n",
       "      <td>5</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id prompt_id                                               text  \\\n",
       "0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n",
       "1  0020ae56ffbf    ebad26  They would rub it up with soda to make the sme...   \n",
       "2  004e978e639e    3b9047  In Egypt, there were many occupations and soci...   \n",
       "3  005ab0199905    3b9047  The highest class was Pharaohs these people we...   \n",
       "4  0070c9e7af47    814d6b  The Third Wave developed  rapidly because the ...   \n",
       "\n",
       "    content   wording  summary_length  \\\n",
       "0  0.205683  0.380538              64   \n",
       "1 -0.548304  0.506755              54   \n",
       "2  3.128928  4.231226             269   \n",
       "3 -0.210614 -0.471415              28   \n",
       "4  3.272894  3.219757             232   \n",
       "\n",
       "                                  fixed_summary_text  splling_err_num  \\\n",
       "0  The third wave was an experimental see how peo...                5   \n",
       "1  They would rub it up with soda to make the sme...                2   \n",
       "2  In Egypt, there were many occupations and soci...               32   \n",
       "3  The highest class was Pharaohs these people we...                5   \n",
       "4  The Third Wave developed  rapidly because the ...               29   \n",
       "\n",
       "                                     prompt_question  \\\n",
       "0  Summarize how the Third Wave developed over su...   \n",
       "1  Summarize the various ways the factory would u...   \n",
       "2  In complete sentences, summarize the structure...   \n",
       "3  In complete sentences, summarize the structure...   \n",
       "4  Summarize how the Third Wave developed over su...   \n",
       "\n",
       "                prompt_title  \\\n",
       "0             The Third Wave   \n",
       "1    Excerpt from The Jungle   \n",
       "2  Egyptian Social Structure   \n",
       "3  Egyptian Social Structure   \n",
       "4             The Third Wave   \n",
       "\n",
       "                                         prompt_text  prompt_length  \\\n",
       "0  Background \\r\\nThe Third Wave experiment took ...            660   \n",
       "1  With one member trimming beef in a cannery, an...           1076   \n",
       "2  Egyptian society was structured like a pyramid...            625   \n",
       "3  Egyptian society was structured like a pyramid...            625   \n",
       "4  Background \\r\\nThe Third Wave experiment took ...            660   \n",
       "\n",
       "   word_overlap_count  bigram_overlap_count  bigram_overlap_ratio  \\\n",
       "0                  14                     4              0.063492   \n",
       "1                  18                    22              0.415094   \n",
       "2                  22                    52              0.194030   \n",
       "3                   6                     6              0.222222   \n",
       "4                  23                    27              0.116883   \n",
       "\n",
       "   trigram_overlap_count  trigram_overlap_ratio  quotes_count  fold  \n",
       "0                      0               0.000000             0   3.0  \n",
       "1                     10               0.192308             0   2.0  \n",
       "2                     23               0.086142             2   1.0  \n",
       "3                      5               0.192308             0   1.0  \n",
       "4                      5               0.021739             4   3.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a GroupKFold object with the specified number of splits\n",
    "gkf = GroupKFold(n_splits=CFG.n_splits)\n",
    "\n",
    "# Iterate through the splits and assign fold numbers to validation data\n",
    "for i, (_, val_index) in enumerate(gkf.split(train, groups=train[\"prompt_id\"])):\n",
    "    train.loc[val_index, \"fold\"] = i\n",
    "\n",
    "# Display the first few rows of the training data with fold assignments\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eba6b5",
   "metadata": {
    "papermill": {
     "duration": 0.072542,
     "end_time": "2023-10-09T18:06:05.826224",
     "exception": false,
     "start_time": "2023-10-09T18:06:05.753682",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluation Metrics 📊📈"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dfbb10",
   "metadata": {
    "papermill": {
     "duration": 0.072119,
     "end_time": "2023-10-09T18:06:05.970788",
     "exception": false,
     "start_time": "2023-10-09T18:06:05.898669",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**:\n",
    "\n",
    "\n",
    "1. `compute_metrics(eval_pred)`:\n",
    "   - This function takes in an argument `eval_pred`, which is expected to be a tuple containing predictions and true labels.\n",
    "   - It calculates the Root Mean Squared Error (RMSE) between the predictions and the labels.\n",
    "   - RMSE measures the average magnitude of the errors between predicted values and actual values.\n",
    "   - The calculated RMSE is returned as a dictionary with the key `\"rmse\"`.\n",
    "\n",
    "2. `compute_mcrmse(eval_pred)`:\n",
    "   - This function also takes in an argument `eval_pred`, which is expected to be a tuple containing predictions and true labels.\n",
    "   - It calculates the Mean Columnwise Root Mean Squared Error (MCRMSE) based on the predictions and labels.\n",
    "   - MCRMSE is a metric used in the competition and calculates the RMSE for each column (content and wording) and then takes the mean of these RMSE values.\n",
    "   - The calculated MCRMSE is returned as a dictionary with three keys:\n",
    "     - `\"content_rmse\"`: RMSE for the content column.\n",
    "     - `\"wording_rmse\"`: RMSE for the wording column.\n",
    "     - `\"mcrmse\"`: Mean of the column-wise RMSE values.\n",
    "\n",
    "3. `compt_score(content_true, content_pred, wording_true, wording_pred)`:\n",
    "   - This function takes in four arguments: `content_true`, `content_pred`, `wording_true`, and `wording_pred`.\n",
    "   - It calculates the competition score based on the RMSE between true and predicted values for both content and wording.\n",
    "   - The competition score is computed as the average of the RMSE for content and wording.\n",
    "   - The function returns the competition score, which provides an overall evaluation of model performance.\n",
    "\n",
    "These functions are designed to evaluate the performance of a model on the competition's specific metrics, including RMSE and MCRMSE, which  used in regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e098dc1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T18:06:06.119419Z",
     "iopub.status.busy": "2023-10-09T18:06:06.118875Z",
     "iopub.status.idle": "2023-10-09T18:06:06.125327Z",
     "shell.execute_reply": "2023-10-09T18:06:06.124448Z"
    },
    "papermill": {
     "duration": 0.082398,
     "end_time": "2023-10-09T18:06:06.126943",
     "exception": false,
     "start_time": "2023-10-09T18:06:06.044545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to compute root mean squared error (RMSE)\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = mean_squared_error(labels, predictions, squared=False)\n",
    "    \n",
    "    # Return RMSE as a dictionary\n",
    "    return {\"rmse\": rmse}\n",
    "\n",
    "# Define a function to compute mean columnwise root mean squared error (MCRMSE)\n",
    "def compute_mcrmse(eval_pred):\n",
    "    \"\"\"\n",
    "    Calculates mean columnwise root mean squared error\n",
    "    Reference: https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n",
    "    \"\"\"\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    # Calculate column-wise RMSE and mean RMSE\n",
    "    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n",
    "    mcrmse = np.mean(col_rmse)\n",
    "\n",
    "    # Return MCRMSE along with individual column RMSE\n",
    "    return {\n",
    "        \"content_rmse\": col_rmse[0],\n",
    "        \"wording_rmse\": col_rmse[1],\n",
    "        \"mcrmse\": mcrmse,\n",
    "    }\n",
    "\n",
    "# Define a function to compute the competition score\n",
    "def compt_score(content_true, content_pred, wording_true, wording_pred):\n",
    "    # Calculate RMSE for content and wording\n",
    "    content_score = mean_squared_error(content_true, content_pred)**(1/2)\n",
    "    wording_score = mean_squared_error(wording_true, wording_pred)**(1/2)\n",
    "    \n",
    "    # Calculate the competition score as the average of content and wording scores\n",
    "    return (content_score + wording_score) / 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f006e3",
   "metadata": {
    "papermill": {
     "duration": 0.073419,
     "end_time": "2023-10-09T18:06:06.272469",
     "exception": false,
     "start_time": "2023-10-09T18:06:06.199050",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 🔍 Content Score Regressor 📈"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f0bf5a",
   "metadata": {
    "papermill": {
     "duration": 0.071723,
     "end_time": "2023-10-09T18:06:06.417583",
     "exception": false,
     "start_time": "2023-10-09T18:06:06.345860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**:\n",
    "\n",
    "\n",
    "This code defines a Python class named `ContentScoreRegressor`, which  used for fine-tuning a pre-trained model and making predictions for content scores.\n",
    "\n",
    "1. `__init__` method:\n",
    "   - This is the class constructor that initializes the `ContentScoreRegressor` object.\n",
    "   - It takes several parameters including `model_name`, `model_dir`, `target`, `hidden_dropout_prob`, `attention_probs_dropout_prob`, and `max_length`.\n",
    "   - It sets various attributes including `inputs`, `input_col`, `text_cols`, `target_cols`, and model-related attributes like `tokenizer`, `model_config`, and initializes the random seed.\n",
    "\n",
    "2. `tokenize_function` method:\n",
    "   - This method takes a DataFrame `examples` as input, which is expected to contain text data and target labels.\n",
    "   - It tokenizes the text data using the pre-trained tokenizer with specified max length and returns a dictionary with tokenized inputs and labels.\n",
    "\n",
    "3. `tokenize_function_test` method:\n",
    "   - Similar to `tokenize_function`, but used for tokenizing test data without labels.\n",
    "\n",
    "4. `train` method:\n",
    "   - This method is used for fine-tuning a pre-trained model on the training data.\n",
    "   - It takes various training-related parameters like `fold`, `train_df`, `valid_df`, `batch_size`, `learning_rate`, `weight_decay`, `num_train_epochs`, and `save_steps`.\n",
    "   - It concatenates the text columns (e.g., prompt title, prompt question, and fixed summary text) in both training and validation DataFrames to create input text.\n",
    "   - It loads the pre-trained model and configures training arguments.\n",
    "   - It fine-tunes the model using the Hugging Face `Trainer` class and saves the best model to the specified directory.\n",
    "\n",
    "5. `predict` method:\n",
    "   - This method is used for making predictions on test data.\n",
    "   - It takes `test_df` and `fold` as input parameters.\n",
    "   - It concatenates the text columns in the test DataFrame to create input text.\n",
    "   - It loads the pre-trained model for inference and makes predictions using the Hugging Face `Trainer`.\n",
    "   - The predicted content scores are returned as an array.\n",
    "\n",
    "The class is designed to encapsulate the fine-tuning and prediction process for content scores using a pre-trained model. It appears to be used in a machine learning pipeline to train and evaluate models ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73b161d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T18:06:06.563922Z",
     "iopub.status.busy": "2023-10-09T18:06:06.563687Z",
     "iopub.status.idle": "2023-10-09T18:06:06.579974Z",
     "shell.execute_reply": "2023-10-09T18:06:06.579186Z"
    },
    "papermill": {
     "duration": 0.091666,
     "end_time": "2023-10-09T18:06:06.581696",
     "exception": false,
     "start_time": "2023-10-09T18:06:06.490030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a class for the Content Score Regressor\n",
    "class ContentScoreRegressor:\n",
    "    def __init__(self, \n",
    "                model_name: str,\n",
    "                model_dir: str,\n",
    "                target: str,\n",
    "                hidden_dropout_prob: float,\n",
    "                attention_probs_dropout_prob: float,\n",
    "                max_length: int,\n",
    "                ):\n",
    "        # Define input columns and target column\n",
    "        self.inputs = [\"prompt_text\", \"prompt_title\", \"prompt_question\", \"fixed_summary_text\"]\n",
    "        self.input_col = \"input\"\n",
    "        self.text_cols = [self.input_col] \n",
    "        self.target = target\n",
    "        self.target_cols = [target]\n",
    "\n",
    "        # Initialize model-related attributes\n",
    "        self.model_name = model_name\n",
    "        self.model_dir = model_dir\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Initialize tokenizer and model configuration\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(f\"/kaggle/input/{model_name}\")\n",
    "        self.model_config = AutoConfig.from_pretrained(f\"/kaggle/input/{model_name}\")\n",
    "        \n",
    "        # Update model configuration with additional parameters\n",
    "        self.model_config.update({\n",
    "            \"hidden_dropout_prob\": hidden_dropout_prob,\n",
    "            \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n",
    "            \"num_labels\": 1,\n",
    "            \"problem_type\": \"regression\",\n",
    "        })\n",
    "        \n",
    "        # Set a fixed random seed for reproducibility\n",
    "        seed_everything(seed=42)\n",
    "\n",
    "        # Initialize data collator for padding\n",
    "        self.data_collator = DataCollatorWithPadding(\n",
    "            tokenizer=self.tokenizer\n",
    "        )\n",
    "\n",
    "    def tokenize_function(self, examples: pd.DataFrame):\n",
    "        labels = [examples[self.target]]\n",
    "        tokenized = self.tokenizer(examples[self.input_col],\n",
    "                         padding=False,\n",
    "                         truncation=True,\n",
    "                         max_length=self.max_length)\n",
    "        return {\n",
    "            **tokenized,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "    \n",
    "    def tokenize_function_test(self, examples: pd.DataFrame):\n",
    "        tokenized = self.tokenizer(examples[self.input_col],\n",
    "                         padding=False,\n",
    "                         truncation=True,\n",
    "                         max_length=self.max_length)\n",
    "        return tokenized\n",
    "        \n",
    "    def train(self, \n",
    "            fold: int,\n",
    "            train_df: pd.DataFrame,\n",
    "            valid_df: pd.DataFrame,\n",
    "            batch_size: int,\n",
    "            learning_rate: float,\n",
    "            weight_decay: float,\n",
    "            num_train_epochs: float,\n",
    "            save_steps: int,\n",
    "        ) -> None:\n",
    "        \"\"\"Fine-tuning the model\"\"\"\n",
    "        \n",
    "        sep = self.tokenizer.sep_token\n",
    "        \n",
    "        # Create input text by concatenating title, question, and summary\n",
    "        train_df[self.input_col] = (\n",
    "                    train_df[\"prompt_title\"] + sep \n",
    "                    + train_df[\"prompt_question\"] + sep \n",
    "                    + train_df[\"fixed_summary_text\"]\n",
    "                  )\n",
    "\n",
    "        valid_df[self.input_col] = (\n",
    "                    valid_df[\"prompt_title\"] + sep \n",
    "                    + valid_df[\"prompt_question\"] + sep \n",
    "                    + valid_df[\"fixed_summary_text\"]\n",
    "                  )\n",
    "        \n",
    "        # Select relevant columns for training and validation\n",
    "        train_df = train_df[[self.input_col] + self.target_cols]\n",
    "        valid_df = valid_df[[self.input_col] + self.target_cols]\n",
    "        \n",
    "        # Load the pre-trained model for content score prediction\n",
    "        model_content = AutoModelForSequenceClassification.from_pretrained(\n",
    "            f\"/kaggle/input/{self.model_name}\", \n",
    "            config=self.model_config\n",
    "        )\n",
    "\n",
    "        # Create datasets from DataFrames\n",
    "        train_dataset = Dataset.from_pandas(train_df, preserve_index=False) \n",
    "        val_dataset = Dataset.from_pandas(valid_df, preserve_index=False) \n",
    "    \n",
    "        # Tokenize and preprocess the datasets\n",
    "        train_tokenized_datasets = train_dataset.map(self.tokenize_function, batched=False)\n",
    "        val_tokenized_datasets = val_dataset.map(self.tokenize_function, batched=False)\n",
    "\n",
    "        # Define model training arguments\n",
    "        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=model_fold_dir,\n",
    "            load_best_model_at_end=True,\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=8,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            weight_decay=weight_decay,\n",
    "            report_to='none',\n",
    "            greater_is_better=False,\n",
    "            save_strategy=\"steps\",\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=save_steps,\n",
    "            save_steps=save_steps,\n",
    "            metric_for_best_model=\"rmse\",\n",
    "            save_total_limit=1\n",
    "        )\n",
    "\n",
    "        # Create a trainer for model training\n",
    "        trainer = Trainer(\n",
    "            model=model_content,\n",
    "            args=training_args,\n",
    "            train_dataset=train_tokenized_datasets,\n",
    "            eval_dataset=val_tokenized_datasets,\n",
    "            tokenizer=self.tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "            data_collator=self.data_collator\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save the trained model and tokenizer\n",
    "        model_content.save_pretrained(self.model_dir)\n",
    "        self.tokenizer.save_pretrained(self.model_dir)\n",
    "\n",
    "        \n",
    "    def predict(self, \n",
    "                test_df: pd.DataFrame,\n",
    "                fold: int,\n",
    "               ):\n",
    "        \"\"\"Predict content score for test data\"\"\"\n",
    "        \n",
    "        sep = self.tokenizer.sep_token\n",
    "        \n",
    "        # Create input text for test data\n",
    "        in_text = (\n",
    "                    test_df[\"prompt_title\"] + sep \n",
    "                    + test_df[\"prompt_question\"] + sep \n",
    "                    + test_df[\"fixed_summary_text\"]\n",
    "                  )\n",
    "        test_df[self.input_col] = in_text\n",
    "\n",
    "        # Select the relevant columns\n",
    "        test_ = test_df[[self.input_col]]\n",
    "    \n",
    "        # Create a dataset from the test data\n",
    "        test_dataset = Dataset.from_pandas(test_, preserve_index=False) \n",
    "        test_tokenized_dataset = test_dataset.map(self.tokenize_function_test, batched=False)\n",
    "\n",
    "        # Load the trained content score prediction model\n",
    "        model_content = AutoModelForSequenceClassification.from_pretrained(f\"{self.model_dir}\")\n",
    "        model_content.eval()\n",
    "        \n",
    "        # Define model prediction arguments\n",
    "        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n",
    "        test_args = TrainingArguments(\n",
    "            output_dir=model_fold_dir,\n",
    "            do_train = False,\n",
    "            do_predict = True,\n",
    "            per_device_eval_batch_size = 4,\n",
    "            dataloader_drop_last = False,\n",
    "        )\n",
    "\n",
    "        # Initialize a trainer for inference\n",
    "        infer_content = Trainer(\n",
    "                      model = model_content, \n",
    "                      tokenizer=self.tokenizer,\n",
    "                      data_collator=self.data_collator,\n",
    "                      args = test_args)\n",
    "\n",
    "        # Perform predictions\n",
    "        preds = infer_content.predict(test_tokenized_dataset)[0]\n",
    "\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97157391",
   "metadata": {
    "papermill": {
     "duration": 0.071152,
     "end_time": "2023-10-09T18:06:06.724901",
     "exception": false,
     "start_time": "2023-10-09T18:06:06.653749",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training, Validation, and Prediction Functions 🚀📊"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2058b112",
   "metadata": {
    "papermill": {
     "duration": 0.122146,
     "end_time": "2023-10-09T18:06:06.919321",
     "exception": false,
     "start_time": "2023-10-09T18:06:06.797175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**:\n",
    "\n",
    "This code defines several functions for training and evaluating a model using K-Fold cross-validation and making predictions. \n",
    "\n",
    "1. `train_by_fold` function:\n",
    "   - This function is used for training a model by fold using K-Fold cross-validation.\n",
    "   - It takes various parameters including `train_df` (the training data), `model_name`, `target` (the target variable), and several hyperparameters for training.\n",
    "   - It first checks if a directory with the `model_name` exists and deletes it if it does. Then, it creates a new directory.\n",
    "   - It iterates through each fold and splits the training data into training and validation sets.\n",
    "   - For each fold, it creates an instance of the `ContentScoreRegressor` class and trains the model using the specified hyperparameters.\n",
    "   - If `save_each_model` is set to `True`, it saves each model in a separate directory based on the fold.\n",
    "\n",
    "2. `validate` function:\n",
    "   - This function is used for validating the model and making out-of-fold (oof) predictions.\n",
    "   - It takes parameters similar to `train_by_fold` and iterates through each fold.\n",
    "   - For each fold, it creates an instance of the `ContentScoreRegressor` class and makes predictions on the validation data.\n",
    "   - The predictions are then assigned to the corresponding rows in the training DataFrame.\n",
    "   - This function returns the updated training DataFrame with predictions.\n",
    "\n",
    "3. `predict` function:\n",
    "   - This function is used for making predictions on the test data.\n",
    "   - It also takes parameters similar to `train_by_fold` and iterates through each fold.\n",
    "   - For each fold, it creates an instance of the `ContentScoreRegressor` class and makes predictions on the test data.\n",
    "   - The fold-specific predictions are stored in columns named `\"target_pred_fold_{fold}\"`.\n",
    "   - Finally, it calculates the mean of fold predictions for each data point and assigns it to the `\"target\"` column in the test DataFrame.\n",
    "\n",
    "These functions are designed to automate the process of training, validating, and making predictions using K-Fold cross-validation for a regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c84c34c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T18:06:07.095114Z",
     "iopub.status.busy": "2023-10-09T18:06:07.094854Z",
     "iopub.status.idle": "2023-10-09T18:06:07.107563Z",
     "shell.execute_reply": "2023-10-09T18:06:07.106526Z"
    },
    "papermill": {
     "duration": 0.101098,
     "end_time": "2023-10-09T18:06:07.109434",
     "exception": false,
     "start_time": "2023-10-09T18:06:07.008336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function for training by fold\n",
    "def train_by_fold(\n",
    "        train_df: pd.DataFrame,\n",
    "        model_name: str,\n",
    "        target: str,\n",
    "        save_each_model: bool,\n",
    "        n_splits: int,\n",
    "        batch_size: int,\n",
    "        learning_rate: int,\n",
    "        hidden_dropout_prob: float,\n",
    "        attention_probs_dropout_prob: float,\n",
    "        weight_decay: float,\n",
    "        num_train_epochs: int,\n",
    "        save_steps: int,\n",
    "        max_length: int\n",
    "    ):\n",
    "\n",
    "    # Delete old model files\n",
    "    if os.path.exists(model_name):\n",
    "        shutil.rmtree(model_name)\n",
    "    \n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "    for fold in range(n_splits):\n",
    "        print(f\"fold {fold}:\")\n",
    "        \n",
    "        train_data = train_df[train_df[\"fold\"] != fold]\n",
    "        valid_data = train_df[train_df[\"fold\"] == fold]\n",
    "        \n",
    "        if save_each_model == True:\n",
    "            model_dir =  f\"{target}/{model_name}/fold_{fold}\"\n",
    "        else: \n",
    "            model_dir =  f\"{model_name}/fold_{fold}\"\n",
    "\n",
    "        csr = ContentScoreRegressor(\n",
    "            model_name=model_name,\n",
    "            target=target,\n",
    "            model_dir=model_dir, \n",
    "            hidden_dropout_prob=hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        \n",
    "        csr.train(\n",
    "            fold=fold,\n",
    "            train_df=train_data,\n",
    "            valid_df=valid_data, \n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            save_steps=save_steps,\n",
    "        )\n",
    "\n",
    "# Define a function for validation (predicting oof data)\n",
    "def validate(\n",
    "    train_df: pd.DataFrame,\n",
    "    target: str,\n",
    "    save_each_model: bool,\n",
    "    model_name: str,\n",
    "    hidden_dropout_prob: float,\n",
    "    attention_probs_dropout_prob: float,\n",
    "    max_length: int\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"Predict out-of-fold (oof) data\"\"\"\n",
    "    for fold in range(CFG.n_splits):\n",
    "        print(f\"fold {fold}:\")\n",
    "        \n",
    "        valid_data = train_df[train_df[\"fold\"] == fold]\n",
    "        \n",
    "        if save_each_model == True:\n",
    "            model_dir =  f\"{target}/{model_name}/fold_{fold}\"\n",
    "        else: \n",
    "            model_dir =  f\"{model_name}/fold_{fold}\"\n",
    "        \n",
    "        csr = ContentScoreRegressor(\n",
    "            model_name=model_name,\n",
    "            target=target,\n",
    "            model_dir=model_dir,\n",
    "            hidden_dropout_prob=hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        \n",
    "        pred = csr.predict(\n",
    "            test_df=valid_data, \n",
    "            fold=fold\n",
    "        )\n",
    "        \n",
    "        train_df.loc[valid_data.index, f\"{target}_pred\"] = pred\n",
    "\n",
    "    return train_df\n",
    "    \n",
    "# Define a function for prediction (using mean folds)\n",
    "def predict(\n",
    "    test_df: pd.DataFrame,\n",
    "    target: str,\n",
    "    save_each_model: bool,\n",
    "    model_name: str,\n",
    "    hidden_dropout_prob: float,\n",
    "    attention_probs_dropout_prob: float,\n",
    "    max_length: int\n",
    "    ):\n",
    "    \"\"\"Predict using mean of folds\"\"\"\n",
    "\n",
    "    for fold in range(CFG.n_splits):\n",
    "        print(f\"fold {fold}:\")\n",
    "        \n",
    "        if save_each_model == True:\n",
    "            model_dir =  f\"{target}/{model_name}/fold_{fold}\"\n",
    "        else: \n",
    "            model_dir =  f\"{model_name}/fold_{fold}\"\n",
    "\n",
    "        csr = ContentScoreRegressor(\n",
    "            model_name=model_name,\n",
    "            target=target,\n",
    "            model_dir=model_dir, \n",
    "            hidden_dropout_prob=hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        \n",
    "        pred = csr.predict(\n",
    "            test_df=test_df, \n",
    "            fold=fold\n",
    "        )\n",
    "        \n",
    "        test_df[f\"{target}_pred_{fold}\"] = pred\n",
    "    \n",
    "    test_df[f\"{target}\"] = test_df[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n",
    "\n",
    "    return test_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1076c",
   "metadata": {
    "papermill": {
     "duration": 0.078559,
     "end_time": "2023-10-09T18:06:07.271886",
     "exception": false,
     "start_time": "2023-10-09T18:06:07.193327",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training📊, Validation🚀, and Prediction Pipeline 🔄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32e4946",
   "metadata": {
    "papermill": {
     "duration": 0.075331,
     "end_time": "2023-10-09T18:06:07.426195",
     "exception": false,
     "start_time": "2023-10-09T18:06:07.350864",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**:\n",
    "\n",
    "\n",
    "This code snippet performs the following tasks for two target variables, `\"content\"` and `\"wording\"`:\n",
    "\n",
    "1. Training and Validation:\n",
    "   - For each target variable, it calls the `train_by_fold` function to train a model using K-Fold cross-validation.\n",
    "   - The training data `train` is used, along with various hyperparameters and settings from the `CFG` configuration.\n",
    "   - After training, it calls the `validate` function to make out-of-fold predictions on the training data.\n",
    "   - The predictions are stored in columns named `{target}_pred`, where `target` is either `\"content\"` or `\"wording\"`.\n",
    "\n",
    "2. RMSE Calculation:\n",
    "   - After making predictions, it calculates the Root Mean Squared Error (RMSE) between the true target values and the predicted values.\n",
    "   - The calculated RMSE is printed to the console to assess the model's performance on the training data.\n",
    "\n",
    "3. Test Data Prediction:\n",
    "   - Finally, it calls the `predict` function to make predictions on the test data.\n",
    "   - The test data `test` is used, and predictions are stored in columns named `{target}_pred_fold_{fold}` for each fold.\n",
    "   - Then, it calculates the mean of fold predictions for each data point and assigns it to the `{target}` column in the test DataFrame.\n",
    "\n",
    "This code segment essentially trains, validates, and makes predictions for both `\"content\"` and `\"wording\"` target variables using K-Fold cross-validation. It allows for evaluating the model's performance on both the training data and the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dd11e0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T18:06:07.574719Z",
     "iopub.status.busy": "2023-10-09T18:06:07.573972Z",
     "iopub.status.idle": "2023-10-09T21:38:00.534111Z",
     "shell.execute_reply": "2023-10-09T21:38:00.533231Z"
    },
    "papermill": {
     "duration": 12713.036528,
     "end_time": "2023-10-09T21:38:00.536302",
     "exception": false,
     "start_time": "2023-10-09T18:06:07.499774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2130' max='2130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2130/2130 25:07, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.225677</td>\n",
       "      <td>0.475054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.259416</td>\n",
       "      <td>0.509329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.260612</td>\n",
       "      <td>0.510502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.262818</td>\n",
       "      <td>0.512658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.229137</td>\n",
       "      <td>0.478683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.561951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.278937</td>\n",
       "      <td>0.528145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.206014</td>\n",
       "      <td>0.453888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.314481</td>\n",
       "      <td>0.560786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.146500</td>\n",
       "      <td>0.360693</td>\n",
       "      <td>0.600578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.146500</td>\n",
       "      <td>0.326078</td>\n",
       "      <td>0.571033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.146500</td>\n",
       "      <td>0.322497</td>\n",
       "      <td>0.567888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.146500</td>\n",
       "      <td>0.308709</td>\n",
       "      <td>0.555616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.146500</td>\n",
       "      <td>0.305612</td>\n",
       "      <td>0.552822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.093300</td>\n",
       "      <td>0.328215</td>\n",
       "      <td>0.572901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.093300</td>\n",
       "      <td>0.264367</td>\n",
       "      <td>0.514166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.093300</td>\n",
       "      <td>0.274777</td>\n",
       "      <td>0.524192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.093300</td>\n",
       "      <td>0.249670</td>\n",
       "      <td>0.499670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.093300</td>\n",
       "      <td>0.271733</td>\n",
       "      <td>0.521280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.054500</td>\n",
       "      <td>0.276010</td>\n",
       "      <td>0.525366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.054500</td>\n",
       "      <td>0.282120</td>\n",
       "      <td>0.531150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2150' max='2150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2150/2150 25:55, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.301243</td>\n",
       "      <td>0.548856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.338356</td>\n",
       "      <td>0.581684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.299120</td>\n",
       "      <td>0.546919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.367686</td>\n",
       "      <td>0.606371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.248900</td>\n",
       "      <td>0.312858</td>\n",
       "      <td>0.559337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.248900</td>\n",
       "      <td>0.263510</td>\n",
       "      <td>0.513332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.248900</td>\n",
       "      <td>0.266322</td>\n",
       "      <td>0.516064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.248900</td>\n",
       "      <td>0.259660</td>\n",
       "      <td>0.509568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.248900</td>\n",
       "      <td>0.269284</td>\n",
       "      <td>0.518925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.149100</td>\n",
       "      <td>0.282526</td>\n",
       "      <td>0.531531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.149100</td>\n",
       "      <td>0.273347</td>\n",
       "      <td>0.522826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.149100</td>\n",
       "      <td>0.288221</td>\n",
       "      <td>0.536862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.149100</td>\n",
       "      <td>0.299359</td>\n",
       "      <td>0.547137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.149100</td>\n",
       "      <td>0.272145</td>\n",
       "      <td>0.521676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.096900</td>\n",
       "      <td>0.296204</td>\n",
       "      <td>0.544247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.096900</td>\n",
       "      <td>0.292020</td>\n",
       "      <td>0.540389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.096900</td>\n",
       "      <td>0.313830</td>\n",
       "      <td>0.560205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.096900</td>\n",
       "      <td>0.312122</td>\n",
       "      <td>0.558679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.096900</td>\n",
       "      <td>0.284915</td>\n",
       "      <td>0.533774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.064600</td>\n",
       "      <td>0.280603</td>\n",
       "      <td>0.529720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.064600</td>\n",
       "      <td>0.288350</td>\n",
       "      <td>0.536982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2155' max='2155' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2155/2155 25:47, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.241138</td>\n",
       "      <td>0.491058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.239440</td>\n",
       "      <td>0.489326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.220130</td>\n",
       "      <td>0.469180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.262722</td>\n",
       "      <td>0.512564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.256245</td>\n",
       "      <td>0.506206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.237946</td>\n",
       "      <td>0.487798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.324136</td>\n",
       "      <td>0.569329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.232581</td>\n",
       "      <td>0.482266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.223621</td>\n",
       "      <td>0.472886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>0.471999</td>\n",
       "      <td>0.687021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>0.276678</td>\n",
       "      <td>0.526002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>0.227637</td>\n",
       "      <td>0.477113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>0.225289</td>\n",
       "      <td>0.474647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>0.291436</td>\n",
       "      <td>0.539848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>0.238436</td>\n",
       "      <td>0.488299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>0.248135</td>\n",
       "      <td>0.498132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>0.297886</td>\n",
       "      <td>0.545789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>0.301094</td>\n",
       "      <td>0.548720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>0.268589</td>\n",
       "      <td>0.518255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>0.236906</td>\n",
       "      <td>0.486730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>0.278164</td>\n",
       "      <td>0.527413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2530' max='2530' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2530/2530 26:08, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.417230</td>\n",
       "      <td>0.645933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.475577</td>\n",
       "      <td>0.689621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.411541</td>\n",
       "      <td>0.641514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.553685</td>\n",
       "      <td>0.744100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.244500</td>\n",
       "      <td>0.463460</td>\n",
       "      <td>0.680779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.244500</td>\n",
       "      <td>0.532022</td>\n",
       "      <td>0.729398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.244500</td>\n",
       "      <td>0.490783</td>\n",
       "      <td>0.700559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.244500</td>\n",
       "      <td>0.364808</td>\n",
       "      <td>0.603993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.244500</td>\n",
       "      <td>0.328166</td>\n",
       "      <td>0.572858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.149900</td>\n",
       "      <td>0.556829</td>\n",
       "      <td>0.746210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.149900</td>\n",
       "      <td>0.432611</td>\n",
       "      <td>0.657732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.149900</td>\n",
       "      <td>0.531245</td>\n",
       "      <td>0.728866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.149900</td>\n",
       "      <td>0.492297</td>\n",
       "      <td>0.701639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.149900</td>\n",
       "      <td>0.727791</td>\n",
       "      <td>0.853107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.118500</td>\n",
       "      <td>0.550056</td>\n",
       "      <td>0.741657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.118500</td>\n",
       "      <td>0.574520</td>\n",
       "      <td>0.757971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.118500</td>\n",
       "      <td>0.568325</td>\n",
       "      <td>0.753873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.118500</td>\n",
       "      <td>0.554492</td>\n",
       "      <td>0.744642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.118500</td>\n",
       "      <td>0.493200</td>\n",
       "      <td>0.702282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>0.483772</td>\n",
       "      <td>0.695537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>0.597411</td>\n",
       "      <td>0.772924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>0.559235</td>\n",
       "      <td>0.747820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>0.588696</td>\n",
       "      <td>0.767265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>0.526638</td>\n",
       "      <td>0.725699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>0.549974</td>\n",
       "      <td>0.741602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv content rmse: 0.49375361082024116\n",
      "fold 0:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2130' max='2130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2130/2130 25:05, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.380724</td>\n",
       "      <td>0.617028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.324795</td>\n",
       "      <td>0.569908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.345567</td>\n",
       "      <td>0.587850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.312169</td>\n",
       "      <td>0.558721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.427900</td>\n",
       "      <td>0.310955</td>\n",
       "      <td>0.557633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.427900</td>\n",
       "      <td>0.316653</td>\n",
       "      <td>0.562719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.427900</td>\n",
       "      <td>0.295445</td>\n",
       "      <td>0.543549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.427900</td>\n",
       "      <td>0.298158</td>\n",
       "      <td>0.546038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.427900</td>\n",
       "      <td>0.332855</td>\n",
       "      <td>0.576936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.251600</td>\n",
       "      <td>0.315743</td>\n",
       "      <td>0.561910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.251600</td>\n",
       "      <td>0.317087</td>\n",
       "      <td>0.563104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.251600</td>\n",
       "      <td>0.302577</td>\n",
       "      <td>0.550070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.251600</td>\n",
       "      <td>0.323380</td>\n",
       "      <td>0.568666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.251600</td>\n",
       "      <td>0.320881</td>\n",
       "      <td>0.566464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>0.313848</td>\n",
       "      <td>0.560222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>0.324444</td>\n",
       "      <td>0.569600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>0.331825</td>\n",
       "      <td>0.576043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>0.329118</td>\n",
       "      <td>0.573688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>0.326197</td>\n",
       "      <td>0.571136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.086800</td>\n",
       "      <td>0.329941</td>\n",
       "      <td>0.574405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.086800</td>\n",
       "      <td>0.325165</td>\n",
       "      <td>0.570232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2150' max='2150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2150/2150 25:58, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.035232</td>\n",
       "      <td>1.017464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.614542</td>\n",
       "      <td>0.783927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.109254</td>\n",
       "      <td>1.053211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.660439</td>\n",
       "      <td>0.812674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>0.720379</td>\n",
       "      <td>0.848751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>0.596207</td>\n",
       "      <td>0.772144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>1.006268</td>\n",
       "      <td>1.003129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>0.725610</td>\n",
       "      <td>0.851828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>0.682383</td>\n",
       "      <td>0.826065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.225100</td>\n",
       "      <td>0.714115</td>\n",
       "      <td>0.845053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.225100</td>\n",
       "      <td>0.592965</td>\n",
       "      <td>0.770042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.225100</td>\n",
       "      <td>0.719417</td>\n",
       "      <td>0.848184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.225100</td>\n",
       "      <td>0.604073</td>\n",
       "      <td>0.777221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.225100</td>\n",
       "      <td>0.643451</td>\n",
       "      <td>0.802154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.144700</td>\n",
       "      <td>0.732691</td>\n",
       "      <td>0.855974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.144700</td>\n",
       "      <td>0.641614</td>\n",
       "      <td>0.801008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.144700</td>\n",
       "      <td>0.636205</td>\n",
       "      <td>0.797625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.144700</td>\n",
       "      <td>0.643246</td>\n",
       "      <td>0.802026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.144700</td>\n",
       "      <td>0.724292</td>\n",
       "      <td>0.851053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.079900</td>\n",
       "      <td>0.665398</td>\n",
       "      <td>0.815719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.079900</td>\n",
       "      <td>0.688284</td>\n",
       "      <td>0.829629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2155' max='2155' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2155/2155 25:49, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.428227</td>\n",
       "      <td>0.654391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.440664</td>\n",
       "      <td>0.663825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.516201</td>\n",
       "      <td>0.718471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.367005</td>\n",
       "      <td>0.605810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.443700</td>\n",
       "      <td>0.413805</td>\n",
       "      <td>0.643277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.443700</td>\n",
       "      <td>0.307135</td>\n",
       "      <td>0.554197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.443700</td>\n",
       "      <td>0.347124</td>\n",
       "      <td>0.589172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.443700</td>\n",
       "      <td>0.303571</td>\n",
       "      <td>0.550972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.443700</td>\n",
       "      <td>0.366067</td>\n",
       "      <td>0.605035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.279900</td>\n",
       "      <td>0.370841</td>\n",
       "      <td>0.608968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.279900</td>\n",
       "      <td>0.294659</td>\n",
       "      <td>0.542825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.279900</td>\n",
       "      <td>0.350431</td>\n",
       "      <td>0.591972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.279900</td>\n",
       "      <td>0.371677</td>\n",
       "      <td>0.609653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.279900</td>\n",
       "      <td>0.301767</td>\n",
       "      <td>0.549333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.172200</td>\n",
       "      <td>0.369358</td>\n",
       "      <td>0.607748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.172200</td>\n",
       "      <td>0.332957</td>\n",
       "      <td>0.577025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.172200</td>\n",
       "      <td>0.318929</td>\n",
       "      <td>0.564738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.172200</td>\n",
       "      <td>0.335810</td>\n",
       "      <td>0.579491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.172200</td>\n",
       "      <td>0.350172</td>\n",
       "      <td>0.591753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.100600</td>\n",
       "      <td>0.372977</td>\n",
       "      <td>0.610719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.100600</td>\n",
       "      <td>0.348678</td>\n",
       "      <td>0.590490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2530' max='2530' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2530/2530 26:08, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.737400</td>\n",
       "      <td>0.858720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.771909</td>\n",
       "      <td>0.878584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.849965</td>\n",
       "      <td>0.921936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.633935</td>\n",
       "      <td>0.796200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.410200</td>\n",
       "      <td>0.470622</td>\n",
       "      <td>0.686019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.410200</td>\n",
       "      <td>0.551843</td>\n",
       "      <td>0.742861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.410200</td>\n",
       "      <td>0.477634</td>\n",
       "      <td>0.691111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.410200</td>\n",
       "      <td>0.452535</td>\n",
       "      <td>0.672707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.410200</td>\n",
       "      <td>0.432250</td>\n",
       "      <td>0.657458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>0.427040</td>\n",
       "      <td>0.653483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>0.444831</td>\n",
       "      <td>0.666957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>0.433357</td>\n",
       "      <td>0.658298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>0.447076</td>\n",
       "      <td>0.668638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>0.487099</td>\n",
       "      <td>0.697925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.190800</td>\n",
       "      <td>0.465578</td>\n",
       "      <td>0.682333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.190800</td>\n",
       "      <td>0.440927</td>\n",
       "      <td>0.664024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.190800</td>\n",
       "      <td>0.451896</td>\n",
       "      <td>0.672232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.190800</td>\n",
       "      <td>0.436766</td>\n",
       "      <td>0.660883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.190800</td>\n",
       "      <td>0.465413</td>\n",
       "      <td>0.682212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.124900</td>\n",
       "      <td>0.447209</td>\n",
       "      <td>0.668737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.124900</td>\n",
       "      <td>0.445447</td>\n",
       "      <td>0.667418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.124900</td>\n",
       "      <td>0.466956</td>\n",
       "      <td>0.683342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.124900</td>\n",
       "      <td>0.448972</td>\n",
       "      <td>0.670053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.124900</td>\n",
       "      <td>0.457797</td>\n",
       "      <td>0.676607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>0.459442</td>\n",
       "      <td>0.677822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv wording rmse: 0.6315901256726625\n",
      "fold 0:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for target in [\"content\", \"wording\"]:\n",
    "    train_by_fold(\n",
    "        train,\n",
    "        model_name=CFG.model_name,\n",
    "        save_each_model=False,\n",
    "        target=target,\n",
    "        learning_rate=CFG.learning_rate,\n",
    "        hidden_dropout_prob=CFG.hidden_dropout_prob,\n",
    "        attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n",
    "        weight_decay=CFG.weight_decay,\n",
    "        num_train_epochs=CFG.num_train_epochs,\n",
    "        n_splits=CFG.n_splits,\n",
    "        batch_size=CFG.batch_size,\n",
    "        save_steps=CFG.save_steps,\n",
    "        max_length=CFG.max_length\n",
    "    )\n",
    "    \n",
    "    \n",
    "    train = validate(\n",
    "        train,\n",
    "        target=target,\n",
    "        save_each_model=False,\n",
    "        model_name=CFG.model_name,\n",
    "        hidden_dropout_prob=CFG.hidden_dropout_prob,\n",
    "        attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n",
    "        max_length=CFG.max_length\n",
    "    )\n",
    "\n",
    "    rmse = mean_squared_error(train[target], train[f\"{target}_pred\"], squared=False)\n",
    "    print(f\"cv {target} rmse: {rmse}\")\n",
    "\n",
    "    test = predict(\n",
    "        test,\n",
    "        target=target,\n",
    "        save_each_model=False,\n",
    "        model_name=CFG.model_name,\n",
    "        hidden_dropout_prob=CFG.hidden_dropout_prob,\n",
    "        attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n",
    "        max_length=CFG.max_length\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fc0f9e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T21:38:00.761002Z",
     "iopub.status.busy": "2023-10-09T21:38:00.760125Z",
     "iopub.status.idle": "2023-10-09T21:38:00.779465Z",
     "shell.execute_reply": "2023-10-09T21:38:00.778588Z"
    },
    "papermill": {
     "duration": 0.16428,
     "end_time": "2023-10-09T21:38:00.781189",
     "exception": false,
     "start_time": "2023-10-09T21:38:00.616909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>fixed_summary_text</th>\n",
       "      <th>splling_err_num</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>...</th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>word_overlap_count</th>\n",
       "      <th>bigram_overlap_count</th>\n",
       "      <th>bigram_overlap_ratio</th>\n",
       "      <th>trigram_overlap_count</th>\n",
       "      <th>trigram_overlap_ratio</th>\n",
       "      <th>quotes_count</th>\n",
       "      <th>fold</th>\n",
       "      <th>content_pred</th>\n",
       "      <th>wording_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000e8c3c7ddb</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The third wave was an experimentto see how peo...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "      <td>64</td>\n",
       "      <td>The third wave was an experimental see how peo...</td>\n",
       "      <td>5</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>...</td>\n",
       "      <td>660</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>0.063492</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.071145</td>\n",
       "      <td>0.750541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0020ae56ffbf</td>\n",
       "      <td>ebad26</td>\n",
       "      <td>They would rub it up with soda to make the sme...</td>\n",
       "      <td>-0.548304</td>\n",
       "      <td>0.506755</td>\n",
       "      <td>54</td>\n",
       "      <td>They would rub it up with soda to make the sme...</td>\n",
       "      <td>2</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>...</td>\n",
       "      <td>1076</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>10</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.651418</td>\n",
       "      <td>0.009339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>004e978e639e</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>In Egypt, there were many occupations and soci...</td>\n",
       "      <td>3.128928</td>\n",
       "      <td>4.231226</td>\n",
       "      <td>269</td>\n",
       "      <td>In Egypt, there were many occupations and soci...</td>\n",
       "      <td>32</td>\n",
       "      <td>In complete sentences, summarize the structure...</td>\n",
       "      <td>Egyptian Social Structure</td>\n",
       "      <td>...</td>\n",
       "      <td>625</td>\n",
       "      <td>22</td>\n",
       "      <td>52</td>\n",
       "      <td>0.194030</td>\n",
       "      <td>23</td>\n",
       "      <td>0.086142</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.226747</td>\n",
       "      <td>2.151596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>005ab0199905</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>The highest class was Pharaohs these people we...</td>\n",
       "      <td>-0.210614</td>\n",
       "      <td>-0.471415</td>\n",
       "      <td>28</td>\n",
       "      <td>The highest class was Pharaohs these people we...</td>\n",
       "      <td>5</td>\n",
       "      <td>In complete sentences, summarize the structure...</td>\n",
       "      <td>Egyptian Social Structure</td>\n",
       "      <td>...</td>\n",
       "      <td>625</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>5</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.993531</td>\n",
       "      <td>-0.829201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0070c9e7af47</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The Third Wave developed  rapidly because the ...</td>\n",
       "      <td>3.272894</td>\n",
       "      <td>3.219757</td>\n",
       "      <td>232</td>\n",
       "      <td>The Third Wave developed  rapidly because the ...</td>\n",
       "      <td>29</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>...</td>\n",
       "      <td>660</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>0.116883</td>\n",
       "      <td>5</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.738156</td>\n",
       "      <td>2.192420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id prompt_id                                               text  \\\n",
       "0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n",
       "1  0020ae56ffbf    ebad26  They would rub it up with soda to make the sme...   \n",
       "2  004e978e639e    3b9047  In Egypt, there were many occupations and soci...   \n",
       "3  005ab0199905    3b9047  The highest class was Pharaohs these people we...   \n",
       "4  0070c9e7af47    814d6b  The Third Wave developed  rapidly because the ...   \n",
       "\n",
       "    content   wording  summary_length  \\\n",
       "0  0.205683  0.380538              64   \n",
       "1 -0.548304  0.506755              54   \n",
       "2  3.128928  4.231226             269   \n",
       "3 -0.210614 -0.471415              28   \n",
       "4  3.272894  3.219757             232   \n",
       "\n",
       "                                  fixed_summary_text  splling_err_num  \\\n",
       "0  The third wave was an experimental see how peo...                5   \n",
       "1  They would rub it up with soda to make the sme...                2   \n",
       "2  In Egypt, there were many occupations and soci...               32   \n",
       "3  The highest class was Pharaohs these people we...                5   \n",
       "4  The Third Wave developed  rapidly because the ...               29   \n",
       "\n",
       "                                     prompt_question  \\\n",
       "0  Summarize how the Third Wave developed over su...   \n",
       "1  Summarize the various ways the factory would u...   \n",
       "2  In complete sentences, summarize the structure...   \n",
       "3  In complete sentences, summarize the structure...   \n",
       "4  Summarize how the Third Wave developed over su...   \n",
       "\n",
       "                prompt_title  ... prompt_length  word_overlap_count  \\\n",
       "0             The Third Wave  ...           660                  14   \n",
       "1    Excerpt from The Jungle  ...          1076                  18   \n",
       "2  Egyptian Social Structure  ...           625                  22   \n",
       "3  Egyptian Social Structure  ...           625                   6   \n",
       "4             The Third Wave  ...           660                  23   \n",
       "\n",
       "   bigram_overlap_count  bigram_overlap_ratio  trigram_overlap_count  \\\n",
       "0                     4              0.063492                      0   \n",
       "1                    22              0.415094                     10   \n",
       "2                    52              0.194030                     23   \n",
       "3                     6              0.222222                      5   \n",
       "4                    27              0.116883                      5   \n",
       "\n",
       "   trigram_overlap_ratio  quotes_count  fold  content_pred  wording_pred  \n",
       "0               0.000000             0   3.0     -0.071145      0.750541  \n",
       "1               0.192308             0   2.0     -0.651418      0.009339  \n",
       "2               0.086142             2   1.0      2.226747      2.151596  \n",
       "3               0.192308             0   1.0     -0.993531     -0.829201  \n",
       "4               0.021739             4   3.0      1.738156      2.192420  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de532f0",
   "metadata": {
    "papermill": {
     "duration": 0.076244,
     "end_time": "2023-10-09T21:38:00.935398",
     "exception": false,
     "start_time": "2023-10-09T21:38:00.859154",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define Targets and Columns to Drop 📊🔍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e58dcf61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T21:38:01.088993Z",
     "iopub.status.busy": "2023-10-09T21:38:01.088670Z",
     "iopub.status.idle": "2023-10-09T21:38:01.092998Z",
     "shell.execute_reply": "2023-10-09T21:38:01.092100Z"
    },
    "papermill": {
     "duration": 0.083463,
     "end_time": "2023-10-09T21:38:01.094699",
     "exception": false,
     "start_time": "2023-10-09T21:38:01.011236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the target variables\n",
    "targets = [\"content\", \"wording\"]\n",
    "\n",
    "# Define columns to drop from the dataset\n",
    "drop_columns = [\n",
    "    \"fold\", \"student_id\", \"prompt_id\", \"text\", \"fixed_summary_text\",\n",
    "    \"prompt_question\", \"prompt_title\", \"prompt_text\"\n",
    "] + targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6d5d99",
   "metadata": {
    "papermill": {
     "duration": 0.077026,
     "end_time": "2023-10-09T21:38:01.248540",
     "exception": false,
     "start_time": "2023-10-09T21:38:01.171514",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LightGBM Model Training 🌟🔍🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5030bffd",
   "metadata": {
    "papermill": {
     "duration": 0.076042,
     "end_time": "2023-10-09T21:38:01.401109",
     "exception": false,
     "start_time": "2023-10-09T21:38:01.325067",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**:\n",
    "\n",
    "This code  is responsible for training LightGBM (Gradient Boosting) models for the target variables `\"content\"` and `\"wording\"` using K-Fold cross-validation.\n",
    "\n",
    "1. `model_dict = {}`:\n",
    "   - Initializes an empty dictionary called `model_dict` that will be used to store the trained models for each target.\n",
    "\n",
    "2. `for target in targets:`:\n",
    "   - Iterates over the target variables. It seems like `targets` is expected to contain the target variable names, which are `\"content\"` and `\"wording\"` in this case.\n",
    "\n",
    "3. `models = []`:\n",
    "   - Initializes an empty list called `models` to store the LightGBM models for the current target variable.\n",
    "\n",
    "4. `for fold in range(CFG.n_splits):`:\n",
    "   - Initiates a loop that iterates through the K-Fold cross-validation folds (specified by `CFG.n_splits`).\n",
    "\n",
    "5. `X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns)`:\n",
    "   - Filters the training data `train` to exclude the current fold. This creates a training dataset `X_train_cv` that does not include the fold being evaluated.\n",
    "   - The `drop_columns` are columns that are not included in the training data.\n",
    "\n",
    "6. `y_train_cv = train[train[\"fold\"] != fold][target]`:\n",
    "   - Extracts the target variable (`target`) for the training dataset `X_train_cv`.\n",
    "\n",
    "7. `X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)`:\n",
    "   - Filters the training data `train` to include only the current fold. This creates an evaluation dataset `X_eval_cv` that consists of data from the fold being evaluated.\n",
    "\n",
    "8. `y_eval_cv = train[train[\"fold\"] == fold][target]`:\n",
    "   - Extracts the target variable (`target`) for the evaluation dataset `X_eval_cv`.\n",
    "\n",
    "9. `dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)`:\n",
    "   - Creates a LightGBM dataset `dtrain` using the training data `X_train_cv` and associated labels `y_train_cv`.\n",
    "\n",
    "10. `dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)`:\n",
    "    - Creates a LightGBM dataset `dval` using the evaluation data `X_eval_cv` and associated labels `y_eval_cv`.\n",
    "\n",
    "11. `params = {...}`:\n",
    "    - Defines a dictionary `params` that contains LightGBM hyperparameters for the model. These hyperparameters include boosting type, random state, objective (regression), metric (root mean squared error), learning rate, maximum depth, and regularization terms.\n",
    "\n",
    "12. `evaluation_results = {}`:\n",
    "    - Initializes an empty dictionary `evaluation_results` to store evaluation results for the model.\n",
    "\n",
    "13. `model = lgb.train(...)`:\n",
    "    - Trains a LightGBM model using the specified hyperparameters and data.\n",
    "    - The `num_boost_round` parameter specifies the maximum number of boosting rounds.\n",
    "    - Early stopping is implemented using `lgb.early_stopping`, which stops training if the validation metric (RMSE) does not improve for a certain number of rounds.\n",
    "    - Evaluation results are recorded using `lgb.log_evaluation` and `lgb.callback.record_evaluation`.\n",
    "\n",
    "14. `models.append(model)`:\n",
    "    - Appends the trained LightGBM model to the `models` list for the current fold.\n",
    "\n",
    "15. `model_dict[target] = models`:\n",
    "    - Stores the list of trained LightGBM models for the current target variable (`target`) in the `model_dict` dictionary.\n",
    "\n",
    "This code performs K-Fold cross-validation to train multiple LightGBM models for each target variable, with each model corresponding to a different fold. The trained models are stored in `model_dict` for later use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f7673cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T21:38:01.559656Z",
     "iopub.status.busy": "2023-10-09T21:38:01.559426Z",
     "iopub.status.idle": "2023-10-09T21:38:02.475319Z",
     "shell.execute_reply": "2023-10-09T21:38:02.474280Z"
    },
    "papermill": {
     "duration": 0.998979,
     "end_time": "2023-10-09T21:38:02.477003",
     "exception": false,
     "start_time": "2023-10-09T21:38:01.478024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1682\n",
      "[LightGBM] [Info] Number of data points in the train set: 5108, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.017606\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\ttrain's rmse: 0.417794\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[71]\ttrain's rmse: 0.414641\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000544 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1646\n",
      "[LightGBM] [Info] Number of data points in the train set: 5156, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -0.039959\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\ttrain's rmse: 0.49514\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[131]\ttrain's rmse: 0.492434\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000154 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1634\n",
      "[LightGBM] [Info] Number of data points in the train set: 5169, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.013356\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\ttrain's rmse: 0.42834\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[97]\ttrain's rmse: 0.428289\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000696 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1687\n",
      "[LightGBM] [Info] Number of data points in the train set: 6062, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -0.044904\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\ttrain's rmse: 0.504536\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[115]\ttrain's rmse: 0.503371\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000523 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1682\n",
      "[LightGBM] [Info] Number of data points in the train set: 5108, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -0.031791\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\ttrain's rmse: 0.523418\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[79]\ttrain's rmse: 0.522578\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1646\n",
      "[LightGBM] [Info] Number of data points in the train set: 5156, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -0.060941\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\ttrain's rmse: 0.652785\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[73]\ttrain's rmse: 0.64991\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000153 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1634\n",
      "[LightGBM] [Info] Number of data points in the train set: 5169, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.028040\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\ttrain's rmse: 0.506064\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[121]\ttrain's rmse: 0.504209\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000610 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1687\n",
      "[LightGBM] [Info] Number of data points in the train set: 6062, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -0.168933\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\ttrain's rmse: 0.658522\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[200]\ttrain's rmse: 0.65829\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[177]\ttrain's rmse: 0.657461\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty dictionary to store models for each target\n",
    "model_dict = {}\n",
    "\n",
    "# Iterate over the target variables \"content\" and \"wording\"\n",
    "for target in targets:\n",
    "    models = []\n",
    "    \n",
    "    for fold in range(CFG.n_splits):\n",
    "\n",
    "        # Create training and evaluation datasets for LightGBM\n",
    "        X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns)\n",
    "        y_train_cv = train[train[\"fold\"] != fold][target]\n",
    "\n",
    "        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n",
    "        y_eval_cv = train[train[\"fold\"] == fold][target]\n",
    "\n",
    "        dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n",
    "        dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)\n",
    "\n",
    "        # Define LightGBM hyperparameters\n",
    "        params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'random_state': 42,\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'learning_rate': 0.040,\n",
    "            'max_depth': 4,  # 3\n",
    "            'lambda_l1': 0.0,\n",
    "            'lambda_l2': 0.011\n",
    "        }\n",
    "\n",
    "        evaluation_results = {}\n",
    "        # Train the LightGBM model\n",
    "        model = lgb.train(params,\n",
    "                          num_boost_round=20000,\n",
    "                            #categorical_feature = categorical_features,\n",
    "                          valid_names=['train', 'valid'],\n",
    "                          train_set=dtrain,\n",
    "                          valid_sets=dval,\n",
    "                          callbacks=[\n",
    "                              lgb.early_stopping(stopping_rounds=30, verbose=True),\n",
    "                              lgb.log_evaluation(100),\n",
    "                              lgb.callback.record_evaluation(evaluation_results)\n",
    "                            ],\n",
    "                          )\n",
    "        models.append(model)\n",
    "    \n",
    "    model_dict[target] = models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6227e7a7",
   "metadata": {
    "papermill": {
     "duration": 0.080232,
     "end_time": "2023-10-09T21:38:02.641259",
     "exception": false,
     "start_time": "2023-10-09T21:38:02.561027",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Cross-Validation and Evaluation 🔄📈"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e9d70d",
   "metadata": {
    "papermill": {
     "duration": 0.078702,
     "end_time": "2023-10-09T21:38:02.799718",
     "exception": false,
     "start_time": "2023-10-09T21:38:02.721016",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**:\n",
    "\n",
    "\n",
    "This code calculates evaluation metrics for the trained LightGBM models using K-Fold cross-validation and prints the RMSE (Root Mean Squared Error) for each target variable. \n",
    "\n",
    "1. `rmses = []`:\n",
    "   - Initializes an empty list called `rmses` to store the RMSE values for each target variable.\n",
    "\n",
    "2. `for target in targets:`:\n",
    "   - Iterates over the target variables. It seems like `targets` is expected to contain the target variable names, which are `\"content\"` and `\"wording\"` in this case.\n",
    "\n",
    "3. `models = model_dict[target]`:\n",
    "   - Retrieves the list of trained LightGBM models corresponding to the current target variable (`target`) from the `model_dict` dictionary.\n",
    "\n",
    "4. `preds = []` and `trues = []`:\n",
    "   - Initialize empty lists to store the predicted values (`preds`) and true values (`trues`) for the current target variable.\n",
    "\n",
    "5. `for fold, model in enumerate(models):`:\n",
    "   - Iterates over the folds and corresponding models for the current target variable.\n",
    "\n",
    "6. `X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)` and `y_eval_cv = train[train[\"fold\"] == fold][target]`:\n",
    "   - Extracts the evaluation data (`X_eval_cv`) and the corresponding true target values (`y_eval_cv`) for the current fold.\n",
    "\n",
    "7. `pred = model.predict(X_eval_cv)`:\n",
    "   - Uses the trained LightGBM model to make predictions on the evaluation data.\n",
    "\n",
    "8. `trues.extend(y_eval_cv)` and `preds.extend(pred)`:\n",
    "   - Extends the `trues` and `preds` lists with the true and predicted values, respectively, for the current fold.\n",
    "\n",
    "9. `rmse = np.sqrt(mean_squared_error(trues, preds))`:\n",
    "   - Calculates the RMSE (Root Mean Squared Error) by comparing the true values (`trues`) and predicted values (`preds`) for the current target variable.\n",
    "\n",
    "10. `print(f\"{target}_rmse : {rmse}\")`:\n",
    "    - Prints the RMSE value for the current target variable to the console.\n",
    "\n",
    "11. `rmses = rmses + [rmse]`:\n",
    "    - Appends the calculated RMSE value to the `rmses` list.\n",
    "\n",
    "12. After the loop for each target variable, the code calculates and prints the mean RMSE (`mcrmse`) across all target variables by averaging the RMSE values stored in the `rmses` list.\n",
    "\n",
    "This code calculates and reports the RMSE for each target variable and provides an average RMSE across all targets to evaluate the performance of the LightGBM models in a K-Fold cross-validation setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52501b17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T21:38:02.958214Z",
     "iopub.status.busy": "2023-10-09T21:38:02.957987Z",
     "iopub.status.idle": "2023-10-09T21:38:03.060385Z",
     "shell.execute_reply": "2023-10-09T21:38:03.059392Z"
    },
    "papermill": {
     "duration": 0.183433,
     "end_time": "2023-10-09T21:38:03.062070",
     "exception": false,
     "start_time": "2023-10-09T21:38:02.878637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_rmse : 0.4554743587260765\n",
      "wording_rmse : 0.5780979205644814\n",
      "mcrmse : 0.516786139645279\n"
     ]
    }
   ],
   "source": [
    "# Cell Title: Cross-Validation and Evaluation 🔄📊📈\n",
    "\n",
    "# Initialize a list to store RMSE values for each target\n",
    "rmses = []\n",
    "\n",
    "# Iterate over the target variables \"content\" and \"wording\"\n",
    "for target in targets:\n",
    "    models = model_dict[target]\n",
    "\n",
    "    preds = []\n",
    "    trues = []\n",
    "    \n",
    "    # Iterate over folds and evaluate models\n",
    "    for fold, model in enumerate(models):\n",
    "        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n",
    "        y_eval_cv = train[train[\"fold\"] == fold][target]\n",
    "\n",
    "        pred = model.predict(X_eval_cv)\n",
    "\n",
    "        trues.extend(y_eval_cv)\n",
    "        preds.extend(pred)\n",
    "        \n",
    "    # Calculate RMSE for the target\n",
    "    rmse = np.sqrt(mean_squared_error(trues, preds))\n",
    "    print(f\"{target}_rmse : {rmse}\")\n",
    "    rmses = rmses + [rmse]\n",
    "\n",
    "# Calculate and print the mean columnwise RMSE (mcrmse)\n",
    "print(f\"mcrmse : {sum(rmses) / len(rmses)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb200d41",
   "metadata": {
    "papermill": {
     "duration": 0.078969,
     "end_time": "2023-10-09T21:38:03.220899",
     "exception": false,
     "start_time": "2023-10-09T21:38:03.141930",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define Columns to Drop for Test Data 📊🔍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6eb04101",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T21:38:03.379870Z",
     "iopub.status.busy": "2023-10-09T21:38:03.379197Z",
     "iopub.status.idle": "2023-10-09T21:38:03.384313Z",
     "shell.execute_reply": "2023-10-09T21:38:03.383541Z"
    },
    "papermill": {
     "duration": 0.086344,
     "end_time": "2023-10-09T21:38:03.385982",
     "exception": false,
     "start_time": "2023-10-09T21:38:03.299638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define columns to drop from the test dataset\n",
    "drop_columns = [\n",
    "    #\"fold\",\n",
    "    \"student_id\", \"prompt_id\", \"text\", \"fixed_summary_text\",\n",
    "    \"prompt_question\", \"prompt_title\", \"prompt_text\",\n",
    "    \"input\"\n",
    "] + [\n",
    "    f\"content_pred_{i}\" for i in range(CFG.n_splits)\n",
    "] + [\n",
    "    f\"wording_pred_{i}\" for i in range(CFG.n_splits)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a1b566",
   "metadata": {
    "papermill": {
     "duration": 0.081751,
     "end_time": "2023-10-09T21:38:03.546523",
     "exception": false,
     "start_time": "2023-10-09T21:38:03.464772",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generating Predictions for Test Data 🚀📊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24466a67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T21:38:03.708795Z",
     "iopub.status.busy": "2023-10-09T21:38:03.708484Z",
     "iopub.status.idle": "2023-10-09T21:38:03.725646Z",
     "shell.execute_reply": "2023-10-09T21:38:03.724862Z"
    },
    "papermill": {
     "duration": 0.100466,
     "end_time": "2023-10-09T21:38:03.727397",
     "exception": false,
     "start_time": "2023-10-09T21:38:03.626931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store predictions for each target\n",
    "pred_dict = {}\n",
    "\n",
    "# Iterate over the target variables \"content\" and \"wording\"\n",
    "for target in targets:\n",
    "    models = model_dict[target]\n",
    "    preds = []\n",
    "\n",
    "    # Iterate over folds and generate predictions for the test data\n",
    "    for fold, model in enumerate(models):\n",
    "        X_eval_cv = test.drop(columns=drop_columns)\n",
    "\n",
    "        pred = model.predict(X_eval_cv)\n",
    "        preds.append(pred)\n",
    "    \n",
    "    # Store the predictions for the target in the dictionary\n",
    "    pred_dict[target] = preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0b9ff1",
   "metadata": {
    "papermill": {
     "duration": 0.079813,
     "end_time": "2023-10-09T21:38:03.886410",
     "exception": false,
     "start_time": "2023-10-09T21:38:03.806597",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Combining and Averaging Test Predictions 🚀📊📈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa858707",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T21:38:04.044979Z",
     "iopub.status.busy": "2023-10-09T21:38:04.044769Z",
     "iopub.status.idle": "2023-10-09T21:38:04.053905Z",
     "shell.execute_reply": "2023-10-09T21:38:04.052981Z"
    },
    "papermill": {
     "duration": 0.090699,
     "end_time": "2023-10-09T21:38:04.055568",
     "exception": false,
     "start_time": "2023-10-09T21:38:03.964869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterate over the target variables \"content\" and \"wording\"\n",
    "for target in targets:\n",
    "    preds = pred_dict[target]\n",
    "    \n",
    "    # Iterate over the predictions for each fold\n",
    "    for i, pred in enumerate(preds):\n",
    "        test[f\"{target}_pred_{i}\"] = pred\n",
    "\n",
    "    # Calculate the mean prediction for the target across folds and store it in the test dataset\n",
    "    test[target] = test[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5153fd94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T21:38:04.215531Z",
     "iopub.status.busy": "2023-10-09T21:38:04.214920Z",
     "iopub.status.idle": "2023-10-09T21:38:04.231979Z",
     "shell.execute_reply": "2023-10-09T21:38:04.231114Z"
    },
    "papermill": {
     "duration": 0.09844,
     "end_time": "2023-10-09T21:38:04.233571",
     "exception": false,
     "start_time": "2023-10-09T21:38:04.135131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>fixed_summary_text</th>\n",
       "      <th>splling_err_num</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>...</th>\n",
       "      <th>content_pred_0</th>\n",
       "      <th>content_pred_1</th>\n",
       "      <th>content_pred_2</th>\n",
       "      <th>content_pred_3</th>\n",
       "      <th>content</th>\n",
       "      <th>wording_pred_0</th>\n",
       "      <th>wording_pred_1</th>\n",
       "      <th>wording_pred_2</th>\n",
       "      <th>wording_pred_3</th>\n",
       "      <th>wording</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000ffffff</td>\n",
       "      <td>abc123</td>\n",
       "      <td>Example text 1</td>\n",
       "      <td>3</td>\n",
       "      <td>Example text 1</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 1</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.334079</td>\n",
       "      <td>-1.506378</td>\n",
       "      <td>-1.488197</td>\n",
       "      <td>-1.570324</td>\n",
       "      <td>-1.474745</td>\n",
       "      <td>-1.276933</td>\n",
       "      <td>-1.248973</td>\n",
       "      <td>-1.406582</td>\n",
       "      <td>-1.462859</td>\n",
       "      <td>-1.348837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111111eeeeee</td>\n",
       "      <td>def789</td>\n",
       "      <td>Example text 2</td>\n",
       "      <td>3</td>\n",
       "      <td>Example text 2</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 2</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.334079</td>\n",
       "      <td>-1.506378</td>\n",
       "      <td>-1.488197</td>\n",
       "      <td>-1.570324</td>\n",
       "      <td>-1.474745</td>\n",
       "      <td>-1.276933</td>\n",
       "      <td>-1.248973</td>\n",
       "      <td>-1.406582</td>\n",
       "      <td>-1.462859</td>\n",
       "      <td>-1.348837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>222222cccccc</td>\n",
       "      <td>abc123</td>\n",
       "      <td>Example text 3</td>\n",
       "      <td>3</td>\n",
       "      <td>Example text 3</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 1</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.334079</td>\n",
       "      <td>-1.506378</td>\n",
       "      <td>-1.488197</td>\n",
       "      <td>-1.570324</td>\n",
       "      <td>-1.474745</td>\n",
       "      <td>-1.276933</td>\n",
       "      <td>-1.248973</td>\n",
       "      <td>-1.406582</td>\n",
       "      <td>-1.462859</td>\n",
       "      <td>-1.348837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>333333dddddd</td>\n",
       "      <td>def789</td>\n",
       "      <td>Example text 4</td>\n",
       "      <td>3</td>\n",
       "      <td>Example text 4</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 2</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.334079</td>\n",
       "      <td>-1.506378</td>\n",
       "      <td>-1.488197</td>\n",
       "      <td>-1.570324</td>\n",
       "      <td>-1.474745</td>\n",
       "      <td>-1.276933</td>\n",
       "      <td>-1.248973</td>\n",
       "      <td>-1.406582</td>\n",
       "      <td>-1.462859</td>\n",
       "      <td>-1.348837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id prompt_id            text  summary_length fixed_summary_text  \\\n",
       "0  000000ffffff    abc123  Example text 1               3     Example text 1   \n",
       "1  111111eeeeee    def789  Example text 2               3     Example text 2   \n",
       "2  222222cccccc    abc123  Example text 3               3     Example text 3   \n",
       "3  333333dddddd    def789  Example text 4               3     Example text 4   \n",
       "\n",
       "   splling_err_num prompt_question     prompt_title       prompt_text  \\\n",
       "0                0    Summarize...  Example Title 1  Heading\\nText...   \n",
       "1                0    Summarize...  Example Title 2  Heading\\nText...   \n",
       "2                0    Summarize...  Example Title 1  Heading\\nText...   \n",
       "3                0    Summarize...  Example Title 2  Heading\\nText...   \n",
       "\n",
       "   prompt_length  ...  content_pred_0  content_pred_1  content_pred_2  \\\n",
       "0              3  ...       -1.334079       -1.506378       -1.488197   \n",
       "1              3  ...       -1.334079       -1.506378       -1.488197   \n",
       "2              3  ...       -1.334079       -1.506378       -1.488197   \n",
       "3              3  ...       -1.334079       -1.506378       -1.488197   \n",
       "\n",
       "   content_pred_3   content  wording_pred_0 wording_pred_1  wording_pred_2  \\\n",
       "0       -1.570324 -1.474745       -1.276933      -1.248973       -1.406582   \n",
       "1       -1.570324 -1.474745       -1.276933      -1.248973       -1.406582   \n",
       "2       -1.570324 -1.474745       -1.276933      -1.248973       -1.406582   \n",
       "3       -1.570324 -1.474745       -1.276933      -1.248973       -1.406582   \n",
       "\n",
       "   wording_pred_3   wording  \n",
       "0       -1.462859 -1.348837  \n",
       "1       -1.462859 -1.348837  \n",
       "2       -1.462859 -1.348837  \n",
       "3       -1.462859 -1.348837  \n",
       "\n",
       "[4 rows x 27 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fd2c76",
   "metadata": {
    "papermill": {
     "duration": 0.07944,
     "end_time": "2023-10-09T21:38:04.393230",
     "exception": false,
     "start_time": "2023-10-09T21:38:04.313790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##  Submission.csv 📝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dabfe0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T21:38:04.564566Z",
     "iopub.status.busy": "2023-10-09T21:38:04.563759Z",
     "iopub.status.idle": "2023-10-09T21:38:04.573328Z",
     "shell.execute_reply": "2023-10-09T21:38:04.572456Z"
    },
    "papermill": {
     "duration": 0.09573,
     "end_time": "2023-10-09T21:38:04.575038",
     "exception": false,
     "start_time": "2023-10-09T21:38:04.479308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000ffffff</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111111eeeeee</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>222222cccccc</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>333333dddddd</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id  content  wording\n",
       "0  000000ffffff      0.0      0.0\n",
       "1  111111eeeeee      0.0      0.0\n",
       "2  222222cccccc      0.0      0.0\n",
       "3  333333dddddd      0.0      0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9d1c295",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T21:38:04.740230Z",
     "iopub.status.busy": "2023-10-09T21:38:04.740010Z",
     "iopub.status.idle": "2023-10-09T21:38:04.749351Z",
     "shell.execute_reply": "2023-10-09T21:38:04.748460Z"
    },
    "papermill": {
     "duration": 0.092201,
     "end_time": "2023-10-09T21:38:04.751323",
     "exception": false,
     "start_time": "2023-10-09T21:38:04.659122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the final predictions for \"student_id\", \"content\", and \"wording\" to a CSV submission file\n",
    "test[[\"student_id\", \"content\", \"wording\"]].to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269d0c56",
   "metadata": {
    "papermill": {
     "duration": 0.093485,
     "end_time": "2023-10-09T21:38:04.931402",
     "exception": false,
     "start_time": "2023-10-09T21:38:04.837917",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Explore More! 👀\n",
    "Thank you for exploring this notebook! If you found this notebook insightful or if it helped you in any way, I invite you to explore more of my work on my profile.\n",
    "\n",
    "👉 [Visit my Profile](https://www.kaggle.com/zulqarnainali) 👈\n",
    "\n",
    "## Feedback and Gratitude 🙏\n",
    "We value your feedback! Your insights and suggestions are essential for our continuous improvement. If you have any comments, questions, or ideas to share, please don't hesitate to reach out.\n",
    "\n",
    "📬 Contact me via email: [zulqar445ali@gmail.com](mailto:zulqar445ali@gmail.com)\n",
    "\n",
    "I would like to express our heartfelt gratitude for your time and engagement. Your support motivates us to create more valuable content.\n",
    "\n",
    "Happy coding and best of luck in your data science endeavors! 🚀\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13200.627744,
   "end_time": "2023-10-09T21:38:08.661332",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-09T17:58:08.033588",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
